{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "#Your code here\n",
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product) \n",
    "\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot, test_size=1500, random_state=42)\n",
    "\n",
    "#Alternative custom script:\n",
    "# random.seed(123)\n",
    "# test_index = random.sample(range(1,10000), 1500)\n",
    "# test = one_hot_results[test_index]\n",
    "# train = np.delete(one_hot_results, test_index, 0)\n",
    "# label_test = product_onehot[test_index]\n",
    "# label_train = np.delete(product_onehot, test_index, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.9462 - acc: 0.1561 - val_loss: 1.9322 - val_acc: 0.1790\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.9229 - acc: 0.1725 - val_loss: 1.9158 - val_acc: 0.1900\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.9055 - acc: 0.1964 - val_loss: 1.9013 - val_acc: 0.2070\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8888 - acc: 0.2217 - val_loss: 1.8860 - val_acc: 0.2380\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.8710 - acc: 0.2500 - val_loss: 1.8692 - val_acc: 0.2560\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8514 - acc: 0.2791 - val_loss: 1.8508 - val_acc: 0.2790\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8297 - acc: 0.3063 - val_loss: 1.8298 - val_acc: 0.3140\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8052 - acc: 0.3360 - val_loss: 1.8055 - val_acc: 0.3380\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7775 - acc: 0.3656 - val_loss: 1.7779 - val_acc: 0.3580\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7460 - acc: 0.3945 - val_loss: 1.7472 - val_acc: 0.3850\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.7108 - acc: 0.4193 - val_loss: 1.7120 - val_acc: 0.4220\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.6715 - acc: 0.4452 - val_loss: 1.6735 - val_acc: 0.4470\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.6290 - acc: 0.4736 - val_loss: 1.6326 - val_acc: 0.4690\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.5836 - acc: 0.4965 - val_loss: 1.5886 - val_acc: 0.4840\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5360 - acc: 0.5164 - val_loss: 1.5426 - val_acc: 0.5060\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4868 - acc: 0.5356 - val_loss: 1.4957 - val_acc: 0.5290\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4362 - acc: 0.5536 - val_loss: 1.4494 - val_acc: 0.5510\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3861 - acc: 0.5759 - val_loss: 1.4015 - val_acc: 0.5600\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3367 - acc: 0.5961 - val_loss: 1.3550 - val_acc: 0.5860\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2882 - acc: 0.6148 - val_loss: 1.3099 - val_acc: 0.5970\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2420 - acc: 0.6308 - val_loss: 1.2678 - val_acc: 0.6080\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1978 - acc: 0.6445 - val_loss: 1.2277 - val_acc: 0.6200\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.1560 - acc: 0.6552 - val_loss: 1.1910 - val_acc: 0.6250\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1166 - acc: 0.6705 - val_loss: 1.1575 - val_acc: 0.6250\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0799 - acc: 0.6769 - val_loss: 1.1233 - val_acc: 0.6330\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0450 - acc: 0.6873 - val_loss: 1.0904 - val_acc: 0.6490\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0125 - acc: 0.6968 - val_loss: 1.0646 - val_acc: 0.6480\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9822 - acc: 0.7019 - val_loss: 1.0363 - val_acc: 0.6560\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9541 - acc: 0.7083 - val_loss: 1.0119 - val_acc: 0.6640\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9276 - acc: 0.7161 - val_loss: 0.9900 - val_acc: 0.6620\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9030 - acc: 0.7205 - val_loss: 0.9680 - val_acc: 0.6710\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8801 - acc: 0.7276 - val_loss: 0.9472 - val_acc: 0.6720\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8585 - acc: 0.7307 - val_loss: 0.9284 - val_acc: 0.6740\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8385 - acc: 0.7359 - val_loss: 0.9168 - val_acc: 0.6740\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8204 - acc: 0.7389 - val_loss: 0.9017 - val_acc: 0.6840\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8031 - acc: 0.7440 - val_loss: 0.8840 - val_acc: 0.6770\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.7866 - acc: 0.7477 - val_loss: 0.8720 - val_acc: 0.6860\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.7715 - acc: 0.7509 - val_loss: 0.8628 - val_acc: 0.6900\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.7578 - acc: 0.7557 - val_loss: 0.8478 - val_acc: 0.6860\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7439 - acc: 0.7568 - val_loss: 0.8413 - val_acc: 0.6900\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7318 - acc: 0.7587 - val_loss: 0.8283 - val_acc: 0.7010\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7201 - acc: 0.7655 - val_loss: 0.8217 - val_acc: 0.7030\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7092 - acc: 0.7681 - val_loss: 0.8110 - val_acc: 0.7020\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.6986 - acc: 0.7693 - val_loss: 0.8033 - val_acc: 0.7030\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.6890 - acc: 0.7719 - val_loss: 0.7967 - val_acc: 0.7110\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.6797 - acc: 0.7728 - val_loss: 0.7898 - val_acc: 0.7080\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6704 - acc: 0.7757 - val_loss: 0.7855 - val_acc: 0.7020\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.6622 - acc: 0.7783 - val_loss: 0.7802 - val_acc: 0.7090\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.6535 - acc: 0.7812 - val_loss: 0.7767 - val_acc: 0.6990\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.6468 - acc: 0.7831 - val_loss: 0.7689 - val_acc: 0.7100\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.6388 - acc: 0.7840 - val_loss: 0.7615 - val_acc: 0.7260\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.6316 - acc: 0.7867 - val_loss: 0.7606 - val_acc: 0.7160\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6247 - acc: 0.7876 - val_loss: 0.7570 - val_acc: 0.7200\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.6184 - acc: 0.7889 - val_loss: 0.7493 - val_acc: 0.7310\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.6118 - acc: 0.7931 - val_loss: 0.7439 - val_acc: 0.7310\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.6055 - acc: 0.7944 - val_loss: 0.7454 - val_acc: 0.7250\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.5996 - acc: 0.7956 - val_loss: 0.7427 - val_acc: 0.7200\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.5943 - acc: 0.7973 - val_loss: 0.7379 - val_acc: 0.7260\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.5884 - acc: 0.7987 - val_loss: 0.7353 - val_acc: 0.7230\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.5828 - acc: 0.8012 - val_loss: 0.7333 - val_acc: 0.7230\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5777 - acc: 0.8015 - val_loss: 0.7276 - val_acc: 0.7300\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5726 - acc: 0.8060 - val_loss: 0.7257 - val_acc: 0.7270\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5679 - acc: 0.8055 - val_loss: 0.7242 - val_acc: 0.7280\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5635 - acc: 0.8059 - val_loss: 0.7212 - val_acc: 0.7280\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.5585 - acc: 0.8092 - val_loss: 0.7228 - val_acc: 0.7250\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5534 - acc: 0.8100 - val_loss: 0.7159 - val_acc: 0.7430\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.5490 - acc: 0.8127 - val_loss: 0.7159 - val_acc: 0.7340\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.5448 - acc: 0.8145 - val_loss: 0.7146 - val_acc: 0.7320\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5402 - acc: 0.8163 - val_loss: 0.7096 - val_acc: 0.7440\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.5361 - acc: 0.8168 - val_loss: 0.7091 - val_acc: 0.7350\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.5320 - acc: 0.8179 - val_loss: 0.7142 - val_acc: 0.7230\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5283 - acc: 0.8180 - val_loss: 0.7086 - val_acc: 0.7310\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5238 - acc: 0.8200 - val_loss: 0.7036 - val_acc: 0.7440\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.5199 - acc: 0.8239 - val_loss: 0.7097 - val_acc: 0.7340\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.5166 - acc: 0.8235 - val_loss: 0.7018 - val_acc: 0.7370\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.5121 - acc: 0.8259 - val_loss: 0.7017 - val_acc: 0.7380\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.5088 - acc: 0.8256 - val_loss: 0.6984 - val_acc: 0.7440\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5045 - acc: 0.8272 - val_loss: 0.6989 - val_acc: 0.7370\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5011 - acc: 0.8292 - val_loss: 0.6989 - val_acc: 0.7380\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4978 - acc: 0.8304 - val_loss: 0.6944 - val_acc: 0.7430\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.4942 - acc: 0.8323 - val_loss: 0.6965 - val_acc: 0.7400\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4905 - acc: 0.8309 - val_loss: 0.6920 - val_acc: 0.7520\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4867 - acc: 0.8332 - val_loss: 0.6917 - val_acc: 0.7490\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.4834 - acc: 0.8353 - val_loss: 0.6889 - val_acc: 0.7590\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.4802 - acc: 0.8364 - val_loss: 0.6946 - val_acc: 0.7430\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4775 - acc: 0.8373 - val_loss: 0.6918 - val_acc: 0.7420\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.4737 - acc: 0.8396 - val_loss: 0.6883 - val_acc: 0.7520\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4704 - acc: 0.8413 - val_loss: 0.6902 - val_acc: 0.7450\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4675 - acc: 0.8437 - val_loss: 0.6910 - val_acc: 0.7410\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.4643 - acc: 0.8437 - val_loss: 0.6869 - val_acc: 0.7500\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4609 - acc: 0.8441 - val_loss: 0.6913 - val_acc: 0.7430\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.4587 - acc: 0.8448 - val_loss: 0.6889 - val_acc: 0.7410\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.4551 - acc: 0.8479 - val_loss: 0.6846 - val_acc: 0.7510\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.4520 - acc: 0.8500 - val_loss: 0.6857 - val_acc: 0.7540\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.4489 - acc: 0.8503 - val_loss: 0.6841 - val_acc: 0.7530\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4460 - acc: 0.8513 - val_loss: 0.6811 - val_acc: 0.7560\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4430 - acc: 0.8511 - val_loss: 0.6850 - val_acc: 0.7500\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4405 - acc: 0.8501 - val_loss: 0.6824 - val_acc: 0.7660\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.4375 - acc: 0.8540 - val_loss: 0.6828 - val_acc: 0.7530\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.4345 - acc: 0.8552 - val_loss: 0.6829 - val_acc: 0.7550\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.4317 - acc: 0.8561 - val_loss: 0.6844 - val_acc: 0.7580\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.4287 - acc: 0.8583 - val_loss: 0.6841 - val_acc: 0.7530\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4264 - acc: 0.8573 - val_loss: 0.6816 - val_acc: 0.7540\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4233 - acc: 0.8591 - val_loss: 0.6836 - val_acc: 0.7530\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.4209 - acc: 0.8593 - val_loss: 0.6800 - val_acc: 0.7560\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4179 - acc: 0.8621 - val_loss: 0.6884 - val_acc: 0.7430\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4156 - acc: 0.8628 - val_loss: 0.6795 - val_acc: 0.7600\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4126 - acc: 0.8640 - val_loss: 0.6795 - val_acc: 0.7610\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4098 - acc: 0.8648 - val_loss: 0.6818 - val_acc: 0.7560\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.4070 - acc: 0.8661 - val_loss: 0.6794 - val_acc: 0.7590\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.4049 - acc: 0.8677 - val_loss: 0.6798 - val_acc: 0.7560\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4020 - acc: 0.8692 - val_loss: 0.6767 - val_acc: 0.7670\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.3998 - acc: 0.8665 - val_loss: 0.6789 - val_acc: 0.7640\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3972 - acc: 0.8696 - val_loss: 0.6757 - val_acc: 0.7650\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3949 - acc: 0.8715 - val_loss: 0.6784 - val_acc: 0.7650\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.3922 - acc: 0.8731 - val_loss: 0.6786 - val_acc: 0.7630\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.3894 - acc: 0.8725 - val_loss: 0.6777 - val_acc: 0.7630\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3869 - acc: 0.8775 - val_loss: 0.6767 - val_acc: 0.7630\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3849 - acc: 0.8764 - val_loss: 0.6763 - val_acc: 0.7640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3820 - acc: 0.8769 - val_loss: 0.6779 - val_acc: 0.7610\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 59us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 106us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3792914330482483, 0.8786666666666667]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6176439248720805, 0.7600000004768371]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.9387 - acc: 0.1809 - val_loss: 1.9308 - val_acc: 0.1970\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.9213 - acc: 0.2013 - val_loss: 1.9169 - val_acc: 0.2090\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.9067 - acc: 0.2136 - val_loss: 1.9035 - val_acc: 0.2210\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.8915 - acc: 0.2229 - val_loss: 1.8893 - val_acc: 0.2340\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8749 - acc: 0.2364 - val_loss: 1.8725 - val_acc: 0.2420\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8557 - acc: 0.2479 - val_loss: 1.8537 - val_acc: 0.2440\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.8327 - acc: 0.2609 - val_loss: 1.8307 - val_acc: 0.2670\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.8047 - acc: 0.2841 - val_loss: 1.8030 - val_acc: 0.2790\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.7722 - acc: 0.3048 - val_loss: 1.7705 - val_acc: 0.3030\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7352 - acc: 0.3347 - val_loss: 1.7345 - val_acc: 0.3430\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.6949 - acc: 0.3636 - val_loss: 1.6956 - val_acc: 0.3650\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6524 - acc: 0.3923 - val_loss: 1.6551 - val_acc: 0.3880\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.6083 - acc: 0.4153 - val_loss: 1.6138 - val_acc: 0.4130\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5639 - acc: 0.4425 - val_loss: 1.5724 - val_acc: 0.4330\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.5195 - acc: 0.4728 - val_loss: 1.5319 - val_acc: 0.4500\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4751 - acc: 0.5017 - val_loss: 1.4909 - val_acc: 0.4690\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4312 - acc: 0.5223 - val_loss: 1.4499 - val_acc: 0.5180\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3878 - acc: 0.5536 - val_loss: 1.4077 - val_acc: 0.5360\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3451 - acc: 0.5771 - val_loss: 1.3685 - val_acc: 0.5520\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3036 - acc: 0.6049 - val_loss: 1.3317 - val_acc: 0.5660\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2627 - acc: 0.6240 - val_loss: 1.2923 - val_acc: 0.5840\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2231 - acc: 0.6411 - val_loss: 1.2565 - val_acc: 0.5990\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1845 - acc: 0.6521 - val_loss: 1.2197 - val_acc: 0.6170\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1472 - acc: 0.6639 - val_loss: 1.1900 - val_acc: 0.6280\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1122 - acc: 0.6752 - val_loss: 1.1567 - val_acc: 0.6420\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0778 - acc: 0.6865 - val_loss: 1.1253 - val_acc: 0.6420\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0455 - acc: 0.6947 - val_loss: 1.0952 - val_acc: 0.6490\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0146 - acc: 0.7040 - val_loss: 1.0719 - val_acc: 0.6580\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9859 - acc: 0.7105 - val_loss: 1.0443 - val_acc: 0.6610\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9583 - acc: 0.7175 - val_loss: 1.0173 - val_acc: 0.6700\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9327 - acc: 0.7208 - val_loss: 0.9947 - val_acc: 0.6770\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9082 - acc: 0.7268 - val_loss: 0.9764 - val_acc: 0.6800\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8852 - acc: 0.7321 - val_loss: 0.9576 - val_acc: 0.6800\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8639 - acc: 0.7365 - val_loss: 0.9386 - val_acc: 0.6880\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8437 - acc: 0.7407 - val_loss: 0.9208 - val_acc: 0.6920\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8247 - acc: 0.7436 - val_loss: 0.9090 - val_acc: 0.6920\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8070 - acc: 0.7473 - val_loss: 0.8908 - val_acc: 0.7000\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7902 - acc: 0.7492 - val_loss: 0.8809 - val_acc: 0.6930\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7745 - acc: 0.7553 - val_loss: 0.8684 - val_acc: 0.7010\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7601 - acc: 0.7592 - val_loss: 0.8569 - val_acc: 0.7090\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.7464 - acc: 0.7599 - val_loss: 0.8431 - val_acc: 0.7130\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7331 - acc: 0.7621 - val_loss: 0.8330 - val_acc: 0.7090\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7212 - acc: 0.7656 - val_loss: 0.8208 - val_acc: 0.7160\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7095 - acc: 0.7693 - val_loss: 0.8172 - val_acc: 0.7170\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6983 - acc: 0.7725 - val_loss: 0.8091 - val_acc: 0.7200\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.6877 - acc: 0.7711 - val_loss: 0.8078 - val_acc: 0.7140\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6781 - acc: 0.7740 - val_loss: 0.7902 - val_acc: 0.7240\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.6687 - acc: 0.7768 - val_loss: 0.7839 - val_acc: 0.7310\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.6599 - acc: 0.7817 - val_loss: 0.7824 - val_acc: 0.7280\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6511 - acc: 0.7815 - val_loss: 0.7741 - val_acc: 0.7300\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6429 - acc: 0.7841 - val_loss: 0.7688 - val_acc: 0.7340\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6353 - acc: 0.7865 - val_loss: 0.7636 - val_acc: 0.7290\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.6277 - acc: 0.7888 - val_loss: 0.7584 - val_acc: 0.7330\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6208 - acc: 0.7931 - val_loss: 0.7580 - val_acc: 0.7290\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.6141 - acc: 0.7916 - val_loss: 0.7526 - val_acc: 0.7340\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6074 - acc: 0.7951 - val_loss: 0.7501 - val_acc: 0.7330\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.6010 - acc: 0.7960 - val_loss: 0.7444 - val_acc: 0.7430\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5949 - acc: 0.7989 - val_loss: 0.7421 - val_acc: 0.7330\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5888 - acc: 0.8000 - val_loss: 0.7364 - val_acc: 0.7420\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5834 - acc: 0.8001 - val_loss: 0.7336 - val_acc: 0.7460\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 52us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 49us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5782739458243052, 0.8052000000317892]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6632389732201894, 0.7566666668256123]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 2.6013 - acc: 0.1436 - val_loss: 2.5795 - val_acc: 0.1810\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 2.5734 - acc: 0.1827 - val_loss: 2.5610 - val_acc: 0.2030\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 2.5531 - acc: 0.2009 - val_loss: 2.5443 - val_acc: 0.2220\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 2.5340 - acc: 0.2156 - val_loss: 2.5272 - val_acc: 0.2310\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 2.5141 - acc: 0.2292 - val_loss: 2.5083 - val_acc: 0.2460\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 2.4919 - acc: 0.2523 - val_loss: 2.4868 - val_acc: 0.2640\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 2.4665 - acc: 0.2749 - val_loss: 2.4623 - val_acc: 0.2820\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 2.4380 - acc: 0.3011 - val_loss: 2.4345 - val_acc: 0.2970\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 2.4059 - acc: 0.3261 - val_loss: 2.4027 - val_acc: 0.3370\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 2.3703 - acc: 0.3499 - val_loss: 2.3683 - val_acc: 0.3550\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 2.3311 - acc: 0.3733 - val_loss: 2.3297 - val_acc: 0.3750\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 2.2878 - acc: 0.3893 - val_loss: 2.2872 - val_acc: 0.3870\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 2.2411 - acc: 0.4115 - val_loss: 2.2425 - val_acc: 0.4170\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 2.1919 - acc: 0.4369 - val_loss: 2.1962 - val_acc: 0.4390\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 2.1415 - acc: 0.4659 - val_loss: 2.1483 - val_acc: 0.4550\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.0912 - acc: 0.4924 - val_loss: 2.1010 - val_acc: 0.4780\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.0412 - acc: 0.5181 - val_loss: 2.0541 - val_acc: 0.5100\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.9923 - acc: 0.5463 - val_loss: 2.0085 - val_acc: 0.5380\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.9452 - acc: 0.5700 - val_loss: 1.9652 - val_acc: 0.5570\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8997 - acc: 0.5913 - val_loss: 1.9229 - val_acc: 0.5760\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8558 - acc: 0.6155 - val_loss: 1.8817 - val_acc: 0.5830\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8138 - acc: 0.6300 - val_loss: 1.8416 - val_acc: 0.6000\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.7734 - acc: 0.6497 - val_loss: 1.8044 - val_acc: 0.6050\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.7350 - acc: 0.6576 - val_loss: 1.7706 - val_acc: 0.6190\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.6985 - acc: 0.6725 - val_loss: 1.7351 - val_acc: 0.6230\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.6642 - acc: 0.6827 - val_loss: 1.7073 - val_acc: 0.6370\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6315 - acc: 0.6895 - val_loss: 1.6767 - val_acc: 0.6380\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.6012 - acc: 0.6967 - val_loss: 1.6525 - val_acc: 0.6490\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.5727 - acc: 0.7009 - val_loss: 1.6216 - val_acc: 0.6550\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5457 - acc: 0.7059 - val_loss: 1.5981 - val_acc: 0.6590\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.5200 - acc: 0.7107 - val_loss: 1.5769 - val_acc: 0.6710\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4965 - acc: 0.7119 - val_loss: 1.5541 - val_acc: 0.6680\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4742 - acc: 0.7175 - val_loss: 1.5380 - val_acc: 0.6630\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4535 - acc: 0.7204 - val_loss: 1.5162 - val_acc: 0.6710\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.4339 - acc: 0.7243 - val_loss: 1.5004 - val_acc: 0.6730\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.4156 - acc: 0.7273 - val_loss: 1.4852 - val_acc: 0.6760\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.3983 - acc: 0.7300 - val_loss: 1.4698 - val_acc: 0.6860\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3822 - acc: 0.7333 - val_loss: 1.4571 - val_acc: 0.6860\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.3669 - acc: 0.7395 - val_loss: 1.4465 - val_acc: 0.6870\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3525 - acc: 0.7385 - val_loss: 1.4337 - val_acc: 0.6880\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3388 - acc: 0.7413 - val_loss: 1.4237 - val_acc: 0.6890\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3257 - acc: 0.7433 - val_loss: 1.4125 - val_acc: 0.6850\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3137 - acc: 0.7492 - val_loss: 1.3995 - val_acc: 0.7010\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3017 - acc: 0.7507 - val_loss: 1.3908 - val_acc: 0.7040\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2902 - acc: 0.7509 - val_loss: 1.3860 - val_acc: 0.6910\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2797 - acc: 0.7539 - val_loss: 1.3756 - val_acc: 0.7000\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2691 - acc: 0.7545 - val_loss: 1.3646 - val_acc: 0.7030\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2593 - acc: 0.7593 - val_loss: 1.3582 - val_acc: 0.7030\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.2496 - acc: 0.7608 - val_loss: 1.3497 - val_acc: 0.7060\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.2402 - acc: 0.7623 - val_loss: 1.3426 - val_acc: 0.7090\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.2314 - acc: 0.7655 - val_loss: 1.3385 - val_acc: 0.7050\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.2223 - acc: 0.7625 - val_loss: 1.3293 - val_acc: 0.7210\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2144 - acc: 0.7717 - val_loss: 1.3243 - val_acc: 0.7140\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.2075 - acc: 0.773 - 0s 43us/step - loss: 1.2063 - acc: 0.7735 - val_loss: 1.3193 - val_acc: 0.7110\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1979 - acc: 0.7749 - val_loss: 1.3096 - val_acc: 0.7280\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1903 - acc: 0.7749 - val_loss: 1.3049 - val_acc: 0.7260\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.1826 - acc: 0.7799 - val_loss: 1.3003 - val_acc: 0.7220\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.1752 - acc: 0.7811 - val_loss: 1.2966 - val_acc: 0.7210\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.1679 - acc: 0.7823 - val_loss: 1.2895 - val_acc: 0.7280\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1607 - acc: 0.7823 - val_loss: 1.2861 - val_acc: 0.7240\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1537 - acc: 0.7836 - val_loss: 1.2805 - val_acc: 0.7220\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1474 - acc: 0.7887 - val_loss: 1.2776 - val_acc: 0.7220\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1407 - acc: 0.7885 - val_loss: 1.2686 - val_acc: 0.7330\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1342 - acc: 0.7911 - val_loss: 1.2676 - val_acc: 0.7320\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1282 - acc: 0.7908 - val_loss: 1.2608 - val_acc: 0.7300\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1215 - acc: 0.7915 - val_loss: 1.2593 - val_acc: 0.7300\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.1154 - acc: 0.7947 - val_loss: 1.2533 - val_acc: 0.7300\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1093 - acc: 0.7961 - val_loss: 1.2490 - val_acc: 0.7310\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.1034 - acc: 0.7981 - val_loss: 1.2458 - val_acc: 0.7310\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0974 - acc: 0.7977 - val_loss: 1.2404 - val_acc: 0.7300\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.0918 - acc: 0.8003 - val_loss: 1.2355 - val_acc: 0.7300\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0862 - acc: 0.8008 - val_loss: 1.2346 - val_acc: 0.7310\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0807 - acc: 0.8024 - val_loss: 1.2312 - val_acc: 0.7330\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0750 - acc: 0.8051 - val_loss: 1.2268 - val_acc: 0.7320\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0698 - acc: 0.8064 - val_loss: 1.2239 - val_acc: 0.7310\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0643 - acc: 0.8063 - val_loss: 1.2255 - val_acc: 0.7260\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0594 - acc: 0.8073 - val_loss: 1.2159 - val_acc: 0.7320\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0539 - acc: 0.8088 - val_loss: 1.2122 - val_acc: 0.7340\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0490 - acc: 0.8100 - val_loss: 1.2091 - val_acc: 0.7330\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0440 - acc: 0.8103 - val_loss: 1.2097 - val_acc: 0.7340\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0391 - acc: 0.8124 - val_loss: 1.2031 - val_acc: 0.7330\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0342 - acc: 0.8129 - val_loss: 1.1991 - val_acc: 0.7410\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0292 - acc: 0.8181 - val_loss: 1.1951 - val_acc: 0.7390\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0244 - acc: 0.8169 - val_loss: 1.1941 - val_acc: 0.7350\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0191 - acc: 0.8187 - val_loss: 1.1917 - val_acc: 0.7350\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0144 - acc: 0.8188 - val_loss: 1.1903 - val_acc: 0.7330\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0098 - acc: 0.8207 - val_loss: 1.1862 - val_acc: 0.7320\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0053 - acc: 0.8188 - val_loss: 1.1815 - val_acc: 0.7380\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0008 - acc: 0.8199 - val_loss: 1.1816 - val_acc: 0.7420\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9960 - acc: 0.8229 - val_loss: 1.1753 - val_acc: 0.7400\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9923 - acc: 0.8249 - val_loss: 1.1725 - val_acc: 0.7440\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9874 - acc: 0.8245 - val_loss: 1.1711 - val_acc: 0.7430\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9831 - acc: 0.8284 - val_loss: 1.1689 - val_acc: 0.7400\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9786 - acc: 0.8279 - val_loss: 1.1671 - val_acc: 0.7340\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9745 - acc: 0.8284 - val_loss: 1.1634 - val_acc: 0.7410\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9703 - acc: 0.8280 - val_loss: 1.1614 - val_acc: 0.7430\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9659 - acc: 0.8305 - val_loss: 1.1598 - val_acc: 0.7360\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9619 - acc: 0.8297 - val_loss: 1.1601 - val_acc: 0.7430\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9578 - acc: 0.8317 - val_loss: 1.1523 - val_acc: 0.7440\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9537 - acc: 0.8320 - val_loss: 1.1494 - val_acc: 0.7450\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9494 - acc: 0.8355 - val_loss: 1.1484 - val_acc: 0.7400\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9456 - acc: 0.8348 - val_loss: 1.1470 - val_acc: 0.7420\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9414 - acc: 0.8348 - val_loss: 1.1447 - val_acc: 0.7400\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9378 - acc: 0.8387 - val_loss: 1.1447 - val_acc: 0.7480\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9337 - acc: 0.8407 - val_loss: 1.1413 - val_acc: 0.7470\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9297 - acc: 0.8393 - val_loss: 1.1398 - val_acc: 0.7380\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9261 - acc: 0.8396 - val_loss: 1.1386 - val_acc: 0.7420\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9222 - acc: 0.8391 - val_loss: 1.1323 - val_acc: 0.7480\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9182 - acc: 0.8421 - val_loss: 1.1291 - val_acc: 0.7510\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9147 - acc: 0.8436 - val_loss: 1.1298 - val_acc: 0.7450\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9109 - acc: 0.8452 - val_loss: 1.1265 - val_acc: 0.7460\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9071 - acc: 0.8457 - val_loss: 1.1246 - val_acc: 0.7500\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9035 - acc: 0.8473 - val_loss: 1.1235 - val_acc: 0.7410\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9002 - acc: 0.8481 - val_loss: 1.1200 - val_acc: 0.7400\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8965 - acc: 0.8472 - val_loss: 1.1165 - val_acc: 0.7490\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8927 - acc: 0.8504 - val_loss: 1.1151 - val_acc: 0.7470\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8893 - acc: 0.8488 - val_loss: 1.1149 - val_acc: 0.7480\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8858 - acc: 0.8503 - val_loss: 1.1177 - val_acc: 0.7420\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8824 - acc: 0.8504 - val_loss: 1.1153 - val_acc: 0.7470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8789 - acc: 0.8528 - val_loss: 1.1095 - val_acc: 0.7460\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 15.9882 - acc: 0.1503 - val_loss: 15.5735 - val_acc: 0.1700\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 15.2171 - acc: 0.1989 - val_loss: 14.8242 - val_acc: 0.2040\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 14.4770 - acc: 0.2284 - val_loss: 14.0997 - val_acc: 0.2290\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 13.7597 - acc: 0.2525 - val_loss: 13.3960 - val_acc: 0.2470\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 13.0619 - acc: 0.2779 - val_loss: 12.7118 - val_acc: 0.2640\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 12.3822 - acc: 0.3100 - val_loss: 12.0471 - val_acc: 0.2990\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 11.7217 - acc: 0.3437 - val_loss: 11.4016 - val_acc: 0.3320\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 11.0808 - acc: 0.3836 - val_loss: 10.7750 - val_acc: 0.3760\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 10.4597 - acc: 0.4272 - val_loss: 10.1690 - val_acc: 0.4190\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 9.8588 - acc: 0.4673 - val_loss: 9.5814 - val_acc: 0.4620\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 9.2786 - acc: 0.5087 - val_loss: 9.0156 - val_acc: 0.4960\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 8.7201 - acc: 0.5385 - val_loss: 8.4708 - val_acc: 0.5300\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 8.1843 - acc: 0.5712 - val_loss: 7.9502 - val_acc: 0.5580\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 7.6708 - acc: 0.6028 - val_loss: 7.4506 - val_acc: 0.5830\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 7.1804 - acc: 0.6265 - val_loss: 6.9754 - val_acc: 0.6000\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 6.7136 - acc: 0.6455 - val_loss: 6.5233 - val_acc: 0.6260\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 6.2709 - acc: 0.6668 - val_loss: 6.0939 - val_acc: 0.6350\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 5.8518 - acc: 0.6776 - val_loss: 5.6915 - val_acc: 0.6500\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 5.4563 - acc: 0.6891 - val_loss: 5.3063 - val_acc: 0.6510\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 5.0837 - acc: 0.6951 - val_loss: 4.9469 - val_acc: 0.6560\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 4.7342 - acc: 0.7004 - val_loss: 4.6126 - val_acc: 0.6600\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 4.4078 - acc: 0.7057 - val_loss: 4.2967 - val_acc: 0.6680\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 4.1037 - acc: 0.7105 - val_loss: 4.0038 - val_acc: 0.6790\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 3.8226 - acc: 0.7140 - val_loss: 3.7349 - val_acc: 0.6840\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 3.5633 - acc: 0.7152 - val_loss: 3.4869 - val_acc: 0.6800\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 3.3260 - acc: 0.7161 - val_loss: 3.2602 - val_acc: 0.6810\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 3.1096 - acc: 0.7188 - val_loss: 3.0557 - val_acc: 0.6900\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.9154 - acc: 0.7177 - val_loss: 2.8733 - val_acc: 0.6780\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.7421 - acc: 0.7199 - val_loss: 2.7081 - val_acc: 0.6880\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 2.5895 - acc: 0.7203 - val_loss: 2.5651 - val_acc: 0.6890\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 2.4574 - acc: 0.7189 - val_loss: 2.4433 - val_acc: 0.6870\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.3443 - acc: 0.7189 - val_loss: 2.3398 - val_acc: 0.6900\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.2505 - acc: 0.7179 - val_loss: 2.2547 - val_acc: 0.6890\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 2.1746 - acc: 0.7168 - val_loss: 2.1866 - val_acc: 0.6870\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 2.1145 - acc: 0.7147 - val_loss: 2.1373 - val_acc: 0.6870\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 2.0702 - acc: 0.7135 - val_loss: 2.0974 - val_acc: 0.6880\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.0364 - acc: 0.7133 - val_loss: 2.0678 - val_acc: 0.6900\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 2.0096 - acc: 0.7128 - val_loss: 2.0427 - val_acc: 0.6890\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.9860 - acc: 0.7129 - val_loss: 2.0191 - val_acc: 0.6860\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.9659 - acc: 0.7124 - val_loss: 1.9996 - val_acc: 0.6820\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.9467 - acc: 0.7116 - val_loss: 1.9820 - val_acc: 0.6850\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.9293 - acc: 0.7115 - val_loss: 1.9643 - val_acc: 0.6840\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.9126 - acc: 0.7121 - val_loss: 1.9499 - val_acc: 0.6890\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.8977 - acc: 0.7116 - val_loss: 1.9334 - val_acc: 0.6850\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.8826 - acc: 0.7129 - val_loss: 1.9204 - val_acc: 0.6830\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8685 - acc: 0.7113 - val_loss: 1.9062 - val_acc: 0.6820\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.8553 - acc: 0.7119 - val_loss: 1.8932 - val_acc: 0.6830\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.8423 - acc: 0.7119 - val_loss: 1.8811 - val_acc: 0.6850\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.8297 - acc: 0.7129 - val_loss: 1.8666 - val_acc: 0.6850\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8174 - acc: 0.7116 - val_loss: 1.8569 - val_acc: 0.6860\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8060 - acc: 0.7119 - val_loss: 1.8459 - val_acc: 0.6880\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.7948 - acc: 0.7105 - val_loss: 1.8344 - val_acc: 0.6860\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.7844 - acc: 0.7109 - val_loss: 1.8231 - val_acc: 0.6870\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.7734 - acc: 0.7099 - val_loss: 1.8143 - val_acc: 0.6850\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.7630 - acc: 0.7119 - val_loss: 1.8014 - val_acc: 0.6870\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7532 - acc: 0.7109 - val_loss: 1.7924 - val_acc: 0.6880\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7430 - acc: 0.7120 - val_loss: 1.7892 - val_acc: 0.6850\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7339 - acc: 0.7109 - val_loss: 1.7731 - val_acc: 0.6840\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.7244 - acc: 0.7119 - val_loss: 1.7628 - val_acc: 0.6830\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.7148 - acc: 0.7132 - val_loss: 1.7538 - val_acc: 0.6850\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7059 - acc: 0.7141 - val_loss: 1.7448 - val_acc: 0.6850\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6969 - acc: 0.7143 - val_loss: 1.7352 - val_acc: 0.6900\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.6882 - acc: 0.7120 - val_loss: 1.7307 - val_acc: 0.6850\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6799 - acc: 0.7125 - val_loss: 1.7236 - val_acc: 0.6780\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6716 - acc: 0.7153 - val_loss: 1.7140 - val_acc: 0.6880\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.6632 - acc: 0.7157 - val_loss: 1.7067 - val_acc: 0.6860\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6549 - acc: 0.7147 - val_loss: 1.6945 - val_acc: 0.6880\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.6471 - acc: 0.7153 - val_loss: 1.6883 - val_acc: 0.6890\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.6390 - acc: 0.7157 - val_loss: 1.6802 - val_acc: 0.6870\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.6314 - acc: 0.7144 - val_loss: 1.6771 - val_acc: 0.6830\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.6244 - acc: 0.7143 - val_loss: 1.6654 - val_acc: 0.6890\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.6166 - acc: 0.7159 - val_loss: 1.6614 - val_acc: 0.6900\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6093 - acc: 0.7164 - val_loss: 1.6527 - val_acc: 0.6890\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.6023 - acc: 0.7157 - val_loss: 1.6435 - val_acc: 0.6870\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5946 - acc: 0.7157 - val_loss: 1.6378 - val_acc: 0.6890\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.5879 - acc: 0.7181 - val_loss: 1.6305 - val_acc: 0.6960\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.5809 - acc: 0.7189 - val_loss: 1.6259 - val_acc: 0.6890\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5742 - acc: 0.7172 - val_loss: 1.6166 - val_acc: 0.6850\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.5675 - acc: 0.7185 - val_loss: 1.6104 - val_acc: 0.6940\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.5604 - acc: 0.7173 - val_loss: 1.6036 - val_acc: 0.6910\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.5539 - acc: 0.7181 - val_loss: 1.6032 - val_acc: 0.6910\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.5476 - acc: 0.7191 - val_loss: 1.5914 - val_acc: 0.6930\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.5410 - acc: 0.7180 - val_loss: 1.5842 - val_acc: 0.6960\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5346 - acc: 0.7199 - val_loss: 1.5777 - val_acc: 0.6900\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.5279 - acc: 0.7193 - val_loss: 1.5710 - val_acc: 0.6950\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.5221 - acc: 0.7215 - val_loss: 1.5649 - val_acc: 0.6940\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.5157 - acc: 0.7205 - val_loss: 1.5583 - val_acc: 0.6980\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5100 - acc: 0.7220 - val_loss: 1.5549 - val_acc: 0.6960\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5038 - acc: 0.7223 - val_loss: 1.5468 - val_acc: 0.6990\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4977 - acc: 0.7215 - val_loss: 1.5439 - val_acc: 0.6980\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4918 - acc: 0.7223 - val_loss: 1.5375 - val_acc: 0.6950\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4865 - acc: 0.7225 - val_loss: 1.5296 - val_acc: 0.6990\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4805 - acc: 0.7236 - val_loss: 1.5269 - val_acc: 0.6950\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4750 - acc: 0.7232 - val_loss: 1.5213 - val_acc: 0.7000\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4695 - acc: 0.7225 - val_loss: 1.5188 - val_acc: 0.6930\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4637 - acc: 0.7253 - val_loss: 1.5090 - val_acc: 0.6960\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4582 - acc: 0.7245 - val_loss: 1.5050 - val_acc: 0.6970\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4529 - acc: 0.7253 - val_loss: 1.5007 - val_acc: 0.6980\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4476 - acc: 0.7251 - val_loss: 1.4900 - val_acc: 0.7010\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4419 - acc: 0.7255 - val_loss: 1.4870 - val_acc: 0.7010\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.4369 - acc: 0.7264 - val_loss: 1.4826 - val_acc: 0.7030\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4309 - acc: 0.7257 - val_loss: 1.4780 - val_acc: 0.6970\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4263 - acc: 0.7264 - val_loss: 1.4715 - val_acc: 0.7040\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4211 - acc: 0.7263 - val_loss: 1.4787 - val_acc: 0.6960\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4163 - acc: 0.7259 - val_loss: 1.4656 - val_acc: 0.6940\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4111 - acc: 0.7264 - val_loss: 1.4563 - val_acc: 0.6990\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4055 - acc: 0.7281 - val_loss: 1.4501 - val_acc: 0.7020\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.4007 - acc: 0.7277 - val_loss: 1.4497 - val_acc: 0.6920\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3960 - acc: 0.7291 - val_loss: 1.4423 - val_acc: 0.7020\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3913 - acc: 0.7301 - val_loss: 1.4370 - val_acc: 0.7010\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3863 - acc: 0.7289 - val_loss: 1.4368 - val_acc: 0.6890\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3814 - acc: 0.7285 - val_loss: 1.4294 - val_acc: 0.7010\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3767 - acc: 0.7308 - val_loss: 1.4215 - val_acc: 0.7040\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3715 - acc: 0.7299 - val_loss: 1.4188 - val_acc: 0.7000\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3666 - acc: 0.7317 - val_loss: 1.4176 - val_acc: 0.7030\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3630 - acc: 0.7299 - val_loss: 1.4095 - val_acc: 0.7050\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3573 - acc: 0.7321 - val_loss: 1.4039 - val_acc: 0.7030\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3526 - acc: 0.7313 - val_loss: 1.4054 - val_acc: 0.6960\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3490 - acc: 0.7309 - val_loss: 1.3948 - val_acc: 0.7060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3436 - acc: 0.7337 - val_loss: 1.3935 - val_acc: 0.7020\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 16.0437 - acc: 0.1733 - val_loss: 15.6437 - val_acc: 0.1770\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 15.2910 - acc: 0.1839 - val_loss: 14.9063 - val_acc: 0.1750\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 14.5623 - acc: 0.1912 - val_loss: 14.1911 - val_acc: 0.1900\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 13.8548 - acc: 0.2001 - val_loss: 13.4962 - val_acc: 0.2030\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 13.1667 - acc: 0.2072 - val_loss: 12.8206 - val_acc: 0.2080\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 12.4975 - acc: 0.2116 - val_loss: 12.1632 - val_acc: 0.2110\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 11.8471 - acc: 0.2151 - val_loss: 11.5244 - val_acc: 0.2110\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 11.2157 - acc: 0.2232 - val_loss: 10.9043 - val_acc: 0.2090\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 10.6036 - acc: 0.2295 - val_loss: 10.3037 - val_acc: 0.2200\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 10.0112 - acc: 0.2392 - val_loss: 9.7210 - val_acc: 0.2490\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 9.4379 - acc: 0.2560 - val_loss: 9.1580 - val_acc: 0.2850\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 8.8840 - acc: 0.2897 - val_loss: 8.6166 - val_acc: 0.2850\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 8.3509 - acc: 0.3029 - val_loss: 8.0963 - val_acc: 0.3040\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 7.8386 - acc: 0.3287 - val_loss: 7.5952 - val_acc: 0.3430\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.3477 - acc: 0.3564 - val_loss: 7.1159 - val_acc: 0.3570\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 6.8783 - acc: 0.3801 - val_loss: 6.6599 - val_acc: 0.3880\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 6.4319 - acc: 0.4029 - val_loss: 6.2255 - val_acc: 0.4110\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 6.0068 - acc: 0.4276 - val_loss: 5.8132 - val_acc: 0.4410\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 5.6032 - acc: 0.4525 - val_loss: 5.4237 - val_acc: 0.4430\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 5.2224 - acc: 0.4671 - val_loss: 5.0561 - val_acc: 0.4780\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 4.8654 - acc: 0.4907 - val_loss: 4.7114 - val_acc: 0.4940\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 4.5306 - acc: 0.5145 - val_loss: 4.3913 - val_acc: 0.5150\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 4.2190 - acc: 0.5387 - val_loss: 4.0917 - val_acc: 0.5310\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 3.9305 - acc: 0.5508 - val_loss: 3.8168 - val_acc: 0.5430\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 3.6651 - acc: 0.5676 - val_loss: 3.5637 - val_acc: 0.5670\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 3.4216 - acc: 0.5857 - val_loss: 3.3320 - val_acc: 0.5850\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 3.1997 - acc: 0.6009 - val_loss: 3.1220 - val_acc: 0.5770\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.9994 - acc: 0.6032 - val_loss: 2.9332 - val_acc: 0.5960\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.8206 - acc: 0.6173 - val_loss: 2.7652 - val_acc: 0.6150\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 2.6635 - acc: 0.6248 - val_loss: 2.6191 - val_acc: 0.6150\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 2.5278 - acc: 0.6303 - val_loss: 2.4947 - val_acc: 0.6200\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.4115 - acc: 0.6335 - val_loss: 2.3879 - val_acc: 0.6290\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.3152 - acc: 0.6393 - val_loss: 2.3026 - val_acc: 0.6270\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 2.2372 - acc: 0.6404 - val_loss: 2.2352 - val_acc: 0.6350\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.1768 - acc: 0.6465 - val_loss: 2.1812 - val_acc: 0.6380\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 2.1314 - acc: 0.6479 - val_loss: 2.1439 - val_acc: 0.6320\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.0972 - acc: 0.6495 - val_loss: 2.1113 - val_acc: 0.6530\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.0694 - acc: 0.6579 - val_loss: 2.0857 - val_acc: 0.6490\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.0454 - acc: 0.6591 - val_loss: 2.0642 - val_acc: 0.6530\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.0231 - acc: 0.6603 - val_loss: 2.0420 - val_acc: 0.6490\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 2.0024 - acc: 0.6624 - val_loss: 2.0214 - val_acc: 0.6540\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.9828 - acc: 0.6660 - val_loss: 2.0059 - val_acc: 0.6560\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.9643 - acc: 0.6676 - val_loss: 1.9841 - val_acc: 0.6630\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.9465 - acc: 0.6727 - val_loss: 1.9696 - val_acc: 0.6620\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.9296 - acc: 0.6763 - val_loss: 1.9504 - val_acc: 0.6670\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.9130 - acc: 0.6777 - val_loss: 1.9345 - val_acc: 0.6660\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8974 - acc: 0.6800 - val_loss: 1.9197 - val_acc: 0.6700\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.8820 - acc: 0.6833 - val_loss: 1.9057 - val_acc: 0.6700\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.8670 - acc: 0.6871 - val_loss: 1.8908 - val_acc: 0.6700\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.8525 - acc: 0.6896 - val_loss: 1.8765 - val_acc: 0.6710\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.8391 - acc: 0.6889 - val_loss: 1.8641 - val_acc: 0.6720\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.8255 - acc: 0.6908 - val_loss: 1.8519 - val_acc: 0.6720\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8127 - acc: 0.6900 - val_loss: 1.8400 - val_acc: 0.6730\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8001 - acc: 0.6924 - val_loss: 1.8252 - val_acc: 0.6690\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.7878 - acc: 0.6920 - val_loss: 1.8130 - val_acc: 0.6690\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.7758 - acc: 0.6931 - val_loss: 1.8022 - val_acc: 0.6720\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.7642 - acc: 0.6944 - val_loss: 1.7899 - val_acc: 0.6700\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7528 - acc: 0.6952 - val_loss: 1.7790 - val_acc: 0.6710\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.7415 - acc: 0.6957 - val_loss: 1.7703 - val_acc: 0.6750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.7314 - acc: 0.6951 - val_loss: 1.7666 - val_acc: 0.6710\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.7208 - acc: 0.6952 - val_loss: 1.7488 - val_acc: 0.6680\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.7104 - acc: 0.6969 - val_loss: 1.7390 - val_acc: 0.6700\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.7002 - acc: 0.6964 - val_loss: 1.7311 - val_acc: 0.6750\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.6905 - acc: 0.6963 - val_loss: 1.7197 - val_acc: 0.6720\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.6807 - acc: 0.6969 - val_loss: 1.7119 - val_acc: 0.6670\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6713 - acc: 0.6980 - val_loss: 1.7006 - val_acc: 0.6690\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.6626 - acc: 0.6960 - val_loss: 1.6920 - val_acc: 0.6770\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.6533 - acc: 0.6977 - val_loss: 1.6858 - val_acc: 0.6720\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.6445 - acc: 0.6984 - val_loss: 1.6743 - val_acc: 0.6720\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.6356 - acc: 0.6979 - val_loss: 1.6656 - val_acc: 0.6750\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.6268 - acc: 0.6992 - val_loss: 1.6584 - val_acc: 0.6750\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.6184 - acc: 0.7004 - val_loss: 1.6526 - val_acc: 0.6730\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.6103 - acc: 0.6993 - val_loss: 1.6422 - val_acc: 0.6840\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.6020 - acc: 0.7004 - val_loss: 1.6365 - val_acc: 0.6730\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.5940 - acc: 0.7012 - val_loss: 1.6280 - val_acc: 0.6800\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.5862 - acc: 0.7005 - val_loss: 1.6187 - val_acc: 0.6750\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.5783 - acc: 0.7020 - val_loss: 1.6114 - val_acc: 0.6770\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.5705 - acc: 0.7036 - val_loss: 1.6050 - val_acc: 0.6790\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5631 - acc: 0.7037 - val_loss: 1.5970 - val_acc: 0.6820\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.5561 - acc: 0.7023 - val_loss: 1.5916 - val_acc: 0.6790\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.5487 - acc: 0.7036 - val_loss: 1.5814 - val_acc: 0.6790\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.5416 - acc: 0.7035 - val_loss: 1.5766 - val_acc: 0.6820\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5343 - acc: 0.7041 - val_loss: 1.5731 - val_acc: 0.6790\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.5280 - acc: 0.7048 - val_loss: 1.5630 - val_acc: 0.6780\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.5206 - acc: 0.7048 - val_loss: 1.5557 - val_acc: 0.6820\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.5142 - acc: 0.7041 - val_loss: 1.5505 - val_acc: 0.6830\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.5070 - acc: 0.7064 - val_loss: 1.5412 - val_acc: 0.6820\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5003 - acc: 0.7067 - val_loss: 1.5355 - val_acc: 0.6810\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4939 - acc: 0.7061 - val_loss: 1.5343 - val_acc: 0.6810\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4874 - acc: 0.7084 - val_loss: 1.5247 - val_acc: 0.6880\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4812 - acc: 0.7076 - val_loss: 1.5197 - val_acc: 0.6850\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4753 - acc: 0.7089 - val_loss: 1.5103 - val_acc: 0.6850\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4683 - acc: 0.7096 - val_loss: 1.5030 - val_acc: 0.6870\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4626 - acc: 0.7099 - val_loss: 1.4992 - val_acc: 0.6850\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4564 - acc: 0.7103 - val_loss: 1.4932 - val_acc: 0.6880\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4500 - acc: 0.7097 - val_loss: 1.4884 - val_acc: 0.6850\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4438 - acc: 0.7111 - val_loss: 1.4814 - val_acc: 0.6880\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4381 - acc: 0.7131 - val_loss: 1.4747 - val_acc: 0.6810\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4321 - acc: 0.7129 - val_loss: 1.4681 - val_acc: 0.6880\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4265 - acc: 0.7129 - val_loss: 1.4637 - val_acc: 0.6870\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4207 - acc: 0.7144 - val_loss: 1.4571 - val_acc: 0.6890\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4152 - acc: 0.7155 - val_loss: 1.4505 - val_acc: 0.6880\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4092 - acc: 0.7149 - val_loss: 1.4457 - val_acc: 0.6880\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4037 - acc: 0.7161 - val_loss: 1.4420 - val_acc: 0.6890\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3985 - acc: 0.7149 - val_loss: 1.4346 - val_acc: 0.6890\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3927 - acc: 0.7165 - val_loss: 1.4304 - val_acc: 0.6910\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3876 - acc: 0.7164 - val_loss: 1.4278 - val_acc: 0.6900\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3826 - acc: 0.7187 - val_loss: 1.4219 - val_acc: 0.6890\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3774 - acc: 0.7183 - val_loss: 1.4128 - val_acc: 0.6920\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3714 - acc: 0.7193 - val_loss: 1.4099 - val_acc: 0.6910\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3664 - acc: 0.7183 - val_loss: 1.4049 - val_acc: 0.6950\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3618 - acc: 0.7195 - val_loss: 1.4010 - val_acc: 0.6920\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3565 - acc: 0.7191 - val_loss: 1.3935 - val_acc: 0.6980\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3516 - acc: 0.7199 - val_loss: 1.3897 - val_acc: 0.6910\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3468 - acc: 0.7200 - val_loss: 1.3859 - val_acc: 0.6980\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3418 - acc: 0.7205 - val_loss: 1.3799 - val_acc: 0.6930\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.3367 - acc: 0.7211 - val_loss: 1.3747 - val_acc: 0.6960\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3324 - acc: 0.7211 - val_loss: 1.3681 - val_acc: 0.6970\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.3272 - acc: 0.7227 - val_loss: 1.3658 - val_acc: 0.6930\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.3230 - acc: 0.7233 - val_loss: 1.3599 - val_acc: 0.6970\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.3180 - acc: 0.7219 - val_loss: 1.3580 - val_acc: 0.6990\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 1s 99us/step - loss: 1.3136 - acc: 0.7225 - val_loss: 1.3539 - val_acc: 0.6950\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 1.3096 - acc: 0.7221 - val_loss: 1.3492 - val_acc: 0.6970\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.3048 - acc: 0.7233 - val_loss: 1.3442 - val_acc: 0.6980\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.3007 - acc: 0.7255 - val_loss: 1.3404 - val_acc: 0.6990\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.2962 - acc: 0.7252 - val_loss: 1.3389 - val_acc: 0.7030\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2921 - acc: 0.7245 - val_loss: 1.3302 - val_acc: 0.7030\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2872 - acc: 0.7248 - val_loss: 1.3327 - val_acc: 0.6990\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.2843 - acc: 0.7256 - val_loss: 1.3232 - val_acc: 0.7000\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2793 - acc: 0.7272 - val_loss: 1.3180 - val_acc: 0.7030\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2752 - acc: 0.7265 - val_loss: 1.3125 - val_acc: 0.7050\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.2710 - acc: 0.7280 - val_loss: 1.3102 - val_acc: 0.7010\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.2672 - acc: 0.7271 - val_loss: 1.3081 - val_acc: 0.7000\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2636 - acc: 0.7277 - val_loss: 1.3014 - val_acc: 0.7050\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2595 - acc: 0.7265 - val_loss: 1.2996 - val_acc: 0.7000\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2559 - acc: 0.7299 - val_loss: 1.2966 - val_acc: 0.7040\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2518 - acc: 0.7295 - val_loss: 1.2919 - val_acc: 0.7010\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.2478 - acc: 0.7292 - val_loss: 1.2879 - val_acc: 0.7000\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2443 - acc: 0.7293 - val_loss: 1.2844 - val_acc: 0.7000\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2411 - acc: 0.7295 - val_loss: 1.2825 - val_acc: 0.6990\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2369 - acc: 0.7292 - val_loss: 1.2778 - val_acc: 0.7060\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2330 - acc: 0.7304 - val_loss: 1.2767 - val_acc: 0.6970\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2296 - acc: 0.7312 - val_loss: 1.2720 - val_acc: 0.7020\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2261 - acc: 0.7297 - val_loss: 1.2663 - val_acc: 0.7030\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2229 - acc: 0.7295 - val_loss: 1.2644 - val_acc: 0.7030\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2193 - acc: 0.7297 - val_loss: 1.2626 - val_acc: 0.7060\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.2156 - acc: 0.7305 - val_loss: 1.2581 - val_acc: 0.7050\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.2130 - acc: 0.7327 - val_loss: 1.2533 - val_acc: 0.7040\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.2090 - acc: 0.7323 - val_loss: 1.2498 - val_acc: 0.6990\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.2060 - acc: 0.7333 - val_loss: 1.2483 - val_acc: 0.7050\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2028 - acc: 0.7308 - val_loss: 1.2433 - val_acc: 0.7060\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1992 - acc: 0.7344 - val_loss: 1.2409 - val_acc: 0.7070\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1964 - acc: 0.7339 - val_loss: 1.2368 - val_acc: 0.7060\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1934 - acc: 0.7317 - val_loss: 1.2342 - val_acc: 0.7120\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1900 - acc: 0.7344 - val_loss: 1.2386 - val_acc: 0.7070\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1878 - acc: 0.7337 - val_loss: 1.2336 - val_acc: 0.7020\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1846 - acc: 0.7355 - val_loss: 1.2235 - val_acc: 0.7070\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1809 - acc: 0.7353 - val_loss: 1.2224 - val_acc: 0.7060\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1781 - acc: 0.7355 - val_loss: 1.2203 - val_acc: 0.7080\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1757 - acc: 0.7341 - val_loss: 1.2174 - val_acc: 0.7100\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1723 - acc: 0.7365 - val_loss: 1.2184 - val_acc: 0.7040\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1701 - acc: 0.7360 - val_loss: 1.2124 - val_acc: 0.7070\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1668 - acc: 0.7355 - val_loss: 1.2078 - val_acc: 0.7100\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1639 - acc: 0.7373 - val_loss: 1.2074 - val_acc: 0.7090\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1615 - acc: 0.7371 - val_loss: 1.2034 - val_acc: 0.7110\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1592 - acc: 0.7368 - val_loss: 1.2063 - val_acc: 0.7100\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1564 - acc: 0.7381 - val_loss: 1.2004 - val_acc: 0.7120\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1539 - acc: 0.7392 - val_loss: 1.1990 - val_acc: 0.7060\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1514 - acc: 0.7375 - val_loss: 1.1939 - val_acc: 0.7130\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1482 - acc: 0.7401 - val_loss: 1.1918 - val_acc: 0.7140\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1462 - acc: 0.7388 - val_loss: 1.1865 - val_acc: 0.7110\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1429 - acc: 0.7384 - val_loss: 1.1867 - val_acc: 0.7140\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1407 - acc: 0.7405 - val_loss: 1.1844 - val_acc: 0.7130\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1385 - acc: 0.7412 - val_loss: 1.1810 - val_acc: 0.7150\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1361 - acc: 0.7397 - val_loss: 1.1809 - val_acc: 0.7160\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1339 - acc: 0.7392 - val_loss: 1.1790 - val_acc: 0.7090\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1317 - acc: 0.7397 - val_loss: 1.1794 - val_acc: 0.7150\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1295 - acc: 0.7408 - val_loss: 1.1730 - val_acc: 0.7150\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1270 - acc: 0.7417 - val_loss: 1.1724 - val_acc: 0.7160\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.1256 - acc: 0.7412 - val_loss: 1.1673 - val_acc: 0.7190\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.1233 - acc: 0.7424 - val_loss: 1.1689 - val_acc: 0.7140\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1208 - acc: 0.7412 - val_loss: 1.1672 - val_acc: 0.7170\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1192 - acc: 0.7428 - val_loss: 1.1637 - val_acc: 0.7160\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1174 - acc: 0.7421 - val_loss: 1.1637 - val_acc: 0.7170\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1151 - acc: 0.7419 - val_loss: 1.1577 - val_acc: 0.7160\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1136 - acc: 0.7423 - val_loss: 1.1597 - val_acc: 0.7170\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1116 - acc: 0.7439 - val_loss: 1.1565 - val_acc: 0.7190\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1096 - acc: 0.7433 - val_loss: 1.1539 - val_acc: 0.7190\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1078 - acc: 0.7435 - val_loss: 1.1559 - val_acc: 0.7130\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1058 - acc: 0.7439 - val_loss: 1.1531 - val_acc: 0.7160\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.1043 - acc: 0.7448 - val_loss: 1.1488 - val_acc: 0.7180\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1026 - acc: 0.7463 - val_loss: 1.1461 - val_acc: 0.7180\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1007 - acc: 0.7440 - val_loss: 1.1479 - val_acc: 0.7180\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0992 - acc: 0.7457 - val_loss: 1.1430 - val_acc: 0.7200\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0975 - acc: 0.7448 - val_loss: 1.1482 - val_acc: 0.7180\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0960 - acc: 0.7439 - val_loss: 1.1402 - val_acc: 0.7170\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0937 - acc: 0.7469 - val_loss: 1.1392 - val_acc: 0.7200\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0923 - acc: 0.7453 - val_loss: 1.1379 - val_acc: 0.7210\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0908 - acc: 0.7447 - val_loss: 1.1358 - val_acc: 0.7180\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0897 - acc: 0.7464 - val_loss: 1.1355 - val_acc: 0.7190\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0877 - acc: 0.7461 - val_loss: 1.1329 - val_acc: 0.7210\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0859 - acc: 0.7456 - val_loss: 1.1319 - val_acc: 0.7200\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0844 - acc: 0.7457 - val_loss: 1.1328 - val_acc: 0.7190\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0830 - acc: 0.7461 - val_loss: 1.1284 - val_acc: 0.7110\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0816 - acc: 0.7463 - val_loss: 1.1288 - val_acc: 0.7210\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0798 - acc: 0.7468 - val_loss: 1.1281 - val_acc: 0.7230\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0789 - acc: 0.7480 - val_loss: 1.1245 - val_acc: 0.7240\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0770 - acc: 0.7471 - val_loss: 1.1246 - val_acc: 0.7270\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0760 - acc: 0.7481 - val_loss: 1.1256 - val_acc: 0.7220\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0741 - acc: 0.7476 - val_loss: 1.1201 - val_acc: 0.7240\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0723 - acc: 0.7472 - val_loss: 1.1186 - val_acc: 0.7180\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0708 - acc: 0.7472 - val_loss: 1.1192 - val_acc: 0.7110\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0697 - acc: 0.7484 - val_loss: 1.1181 - val_acc: 0.7190\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0677 - acc: 0.7475 - val_loss: 1.1183 - val_acc: 0.7180\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0675 - acc: 0.7480 - val_loss: 1.1174 - val_acc: 0.7050\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0654 - acc: 0.7489 - val_loss: 1.1163 - val_acc: 0.7260\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0647 - acc: 0.7495 - val_loss: 1.1136 - val_acc: 0.7110\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0629 - acc: 0.7496 - val_loss: 1.1103 - val_acc: 0.7210\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0616 - acc: 0.7476 - val_loss: 1.1128 - val_acc: 0.7280\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0602 - acc: 0.7484 - val_loss: 1.1106 - val_acc: 0.7240\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0585 - acc: 0.7489 - val_loss: 1.1112 - val_acc: 0.7220\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0581 - acc: 0.7503 - val_loss: 1.1054 - val_acc: 0.7250\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0568 - acc: 0.7499 - val_loss: 1.1080 - val_acc: 0.7280\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0555 - acc: 0.7496 - val_loss: 1.1034 - val_acc: 0.7250\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0540 - acc: 0.7501 - val_loss: 1.1054 - val_acc: 0.7230\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0530 - acc: 0.7503 - val_loss: 1.1002 - val_acc: 0.7220\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0514 - acc: 0.7491 - val_loss: 1.1040 - val_acc: 0.7240\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0509 - acc: 0.7515 - val_loss: 1.1019 - val_acc: 0.7260\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0496 - acc: 0.7503 - val_loss: 1.1003 - val_acc: 0.7170\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0489 - acc: 0.7495 - val_loss: 1.0983 - val_acc: 0.7260\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0475 - acc: 0.7496 - val_loss: 1.0968 - val_acc: 0.7270\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0466 - acc: 0.7503 - val_loss: 1.0943 - val_acc: 0.7260\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0452 - acc: 0.7495 - val_loss: 1.0931 - val_acc: 0.7270\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0441 - acc: 0.7517 - val_loss: 1.0947 - val_acc: 0.7230\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0427 - acc: 0.7492 - val_loss: 1.0957 - val_acc: 0.7230\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0423 - acc: 0.7520 - val_loss: 1.0973 - val_acc: 0.7310\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0407 - acc: 0.7511 - val_loss: 1.0926 - val_acc: 0.7250\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0401 - acc: 0.7517 - val_loss: 1.0906 - val_acc: 0.7290\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0386 - acc: 0.7511 - val_loss: 1.0893 - val_acc: 0.7310\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0379 - acc: 0.7511 - val_loss: 1.0885 - val_acc: 0.7280\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0365 - acc: 0.7525 - val_loss: 1.0879 - val_acc: 0.7240\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0355 - acc: 0.7512 - val_loss: 1.0889 - val_acc: 0.7260\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0347 - acc: 0.7528 - val_loss: 1.0884 - val_acc: 0.7240\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0333 - acc: 0.7529 - val_loss: 1.0853 - val_acc: 0.7270\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.0322 - acc: 0.7524 - val_loss: 1.0838 - val_acc: 0.7260\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0312 - acc: 0.7529 - val_loss: 1.0866 - val_acc: 0.7270\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0302 - acc: 0.7523 - val_loss: 1.0812 - val_acc: 0.7230\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0291 - acc: 0.7523 - val_loss: 1.0826 - val_acc: 0.7230\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0284 - acc: 0.7519 - val_loss: 1.0808 - val_acc: 0.7240\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0270 - acc: 0.7529 - val_loss: 1.0821 - val_acc: 0.7280\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0262 - acc: 0.7520 - val_loss: 1.0801 - val_acc: 0.7290\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0251 - acc: 0.7528 - val_loss: 1.0831 - val_acc: 0.7260\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0236 - acc: 0.7539 - val_loss: 1.0798 - val_acc: 0.7250\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0232 - acc: 0.7527 - val_loss: 1.0814 - val_acc: 0.7290\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0225 - acc: 0.7540 - val_loss: 1.0736 - val_acc: 0.7270\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0211 - acc: 0.7540 - val_loss: 1.0756 - val_acc: 0.7260\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0199 - acc: 0.7540 - val_loss: 1.0744 - val_acc: 0.7320\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0186 - acc: 0.7560 - val_loss: 1.0740 - val_acc: 0.7310\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0178 - acc: 0.7533 - val_loss: 1.0752 - val_acc: 0.7200\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0177 - acc: 0.7563 - val_loss: 1.0747 - val_acc: 0.7310\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.0161 - acc: 0.7571 - val_loss: 1.0702 - val_acc: 0.7230\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0157 - acc: 0.7544 - val_loss: 1.0688 - val_acc: 0.7290\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0150 - acc: 0.7552 - val_loss: 1.0694 - val_acc: 0.7250\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0132 - acc: 0.7563 - val_loss: 1.0672 - val_acc: 0.7290\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0130 - acc: 0.7555 - val_loss: 1.0719 - val_acc: 0.7290\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0120 - acc: 0.7567 - val_loss: 1.0679 - val_acc: 0.7320\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0112 - acc: 0.7561 - val_loss: 1.0658 - val_acc: 0.7270\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0099 - acc: 0.7557 - val_loss: 1.0701 - val_acc: 0.7280\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0098 - acc: 0.7556 - val_loss: 1.0665 - val_acc: 0.7240\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0085 - acc: 0.7577 - val_loss: 1.0647 - val_acc: 0.7340\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0066 - acc: 0.7580 - val_loss: 1.0653 - val_acc: 0.7280\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0059 - acc: 0.7553 - val_loss: 1.0626 - val_acc: 0.7300\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0054 - acc: 0.7585 - val_loss: 1.0649 - val_acc: 0.7270\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0046 - acc: 0.7576 - val_loss: 1.0619 - val_acc: 0.7330\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0039 - acc: 0.7580 - val_loss: 1.0605 - val_acc: 0.7250\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0028 - acc: 0.7568 - val_loss: 1.0694 - val_acc: 0.7320\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0022 - acc: 0.7572 - val_loss: 1.0643 - val_acc: 0.7340\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0011 - acc: 0.7569 - val_loss: 1.0571 - val_acc: 0.7270\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0006 - acc: 0.7565 - val_loss: 1.0579 - val_acc: 0.7280\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9993 - acc: 0.7589 - val_loss: 1.0576 - val_acc: 0.7270\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9983 - acc: 0.7603 - val_loss: 1.0568 - val_acc: 0.7250\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9972 - acc: 0.7592 - val_loss: 1.0592 - val_acc: 0.7330\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9970 - acc: 0.7579 - val_loss: 1.0570 - val_acc: 0.7290\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9959 - acc: 0.7592 - val_loss: 1.0523 - val_acc: 0.7280\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9954 - acc: 0.7571 - val_loss: 1.0508 - val_acc: 0.7290\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9940 - acc: 0.7605 - val_loss: 1.0558 - val_acc: 0.7250\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9938 - acc: 0.7599 - val_loss: 1.0517 - val_acc: 0.7290\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9923 - acc: 0.7600 - val_loss: 1.0513 - val_acc: 0.7310\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9917 - acc: 0.7608 - val_loss: 1.0561 - val_acc: 0.7310\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9911 - acc: 0.7599 - val_loss: 1.0521 - val_acc: 0.7300\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9899 - acc: 0.7607 - val_loss: 1.0586 - val_acc: 0.7260\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9904 - acc: 0.7607 - val_loss: 1.0533 - val_acc: 0.7260\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9880 - acc: 0.7621 - val_loss: 1.0507 - val_acc: 0.7290\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9878 - acc: 0.7613 - val_loss: 1.0463 - val_acc: 0.7310\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9868 - acc: 0.7600 - val_loss: 1.0479 - val_acc: 0.7360\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9858 - acc: 0.7609 - val_loss: 1.0457 - val_acc: 0.7280\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9851 - acc: 0.7632 - val_loss: 1.0444 - val_acc: 0.7270\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9841 - acc: 0.7623 - val_loss: 1.0457 - val_acc: 0.7320\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9830 - acc: 0.7612 - val_loss: 1.0461 - val_acc: 0.7280\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9831 - acc: 0.7616 - val_loss: 1.0438 - val_acc: 0.7360\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9821 - acc: 0.7624 - val_loss: 1.0458 - val_acc: 0.7310\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9815 - acc: 0.7616 - val_loss: 1.0434 - val_acc: 0.7330\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9803 - acc: 0.7627 - val_loss: 1.0430 - val_acc: 0.7280\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9804 - acc: 0.7609 - val_loss: 1.0426 - val_acc: 0.7330\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9787 - acc: 0.7619 - val_loss: 1.0451 - val_acc: 0.7340\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9787 - acc: 0.7619 - val_loss: 1.0392 - val_acc: 0.7350\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9775 - acc: 0.7617 - val_loss: 1.0438 - val_acc: 0.7340\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9772 - acc: 0.7632 - val_loss: 1.0365 - val_acc: 0.7360\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9761 - acc: 0.7613 - val_loss: 1.0380 - val_acc: 0.7370\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9755 - acc: 0.7643 - val_loss: 1.0389 - val_acc: 0.7270\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9742 - acc: 0.7636 - val_loss: 1.0396 - val_acc: 0.7330\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9736 - acc: 0.7651 - val_loss: 1.0379 - val_acc: 0.7280\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9732 - acc: 0.7635 - val_loss: 1.0356 - val_acc: 0.7370\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9725 - acc: 0.7613 - val_loss: 1.0340 - val_acc: 0.7260\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9715 - acc: 0.7623 - val_loss: 1.0403 - val_acc: 0.7250\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9705 - acc: 0.7639 - val_loss: 1.0352 - val_acc: 0.7300\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9705 - acc: 0.7653 - val_loss: 1.0319 - val_acc: 0.7350\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9693 - acc: 0.7640 - val_loss: 1.0369 - val_acc: 0.7350\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9687 - acc: 0.7647 - val_loss: 1.0314 - val_acc: 0.7290\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9676 - acc: 0.7644 - val_loss: 1.0313 - val_acc: 0.7310\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9671 - acc: 0.7649 - val_loss: 1.0313 - val_acc: 0.7360\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9666 - acc: 0.7643 - val_loss: 1.0355 - val_acc: 0.7280\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9654 - acc: 0.7649 - val_loss: 1.0320 - val_acc: 0.7320\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9645 - acc: 0.7651 - val_loss: 1.0279 - val_acc: 0.7360\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9648 - acc: 0.7669 - val_loss: 1.0276 - val_acc: 0.7350\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9638 - acc: 0.7651 - val_loss: 1.0276 - val_acc: 0.7320\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9630 - acc: 0.7685 - val_loss: 1.0313 - val_acc: 0.7330\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9625 - acc: 0.7640 - val_loss: 1.0298 - val_acc: 0.7330\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9618 - acc: 0.7663 - val_loss: 1.0276 - val_acc: 0.7410\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9619 - acc: 0.7635 - val_loss: 1.0250 - val_acc: 0.7410\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9603 - acc: 0.7644 - val_loss: 1.0268 - val_acc: 0.7290\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9599 - acc: 0.7656 - val_loss: 1.0277 - val_acc: 0.7330\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9592 - acc: 0.7664 - val_loss: 1.0293 - val_acc: 0.7310\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9595 - acc: 0.7661 - val_loss: 1.0308 - val_acc: 0.7340\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9580 - acc: 0.7664 - val_loss: 1.0225 - val_acc: 0.7320\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9570 - acc: 0.7661 - val_loss: 1.0227 - val_acc: 0.7310\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9570 - acc: 0.7653 - val_loss: 1.0307 - val_acc: 0.7340\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9556 - acc: 0.7671 - val_loss: 1.0299 - val_acc: 0.7300\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9551 - acc: 0.7671 - val_loss: 1.0262 - val_acc: 0.7300\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9547 - acc: 0.7665 - val_loss: 1.0178 - val_acc: 0.7360\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9542 - acc: 0.7668 - val_loss: 1.0186 - val_acc: 0.7330\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9531 - acc: 0.7660 - val_loss: 1.0361 - val_acc: 0.7260\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9531 - acc: 0.7677 - val_loss: 1.0181 - val_acc: 0.7370\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9523 - acc: 0.7672 - val_loss: 1.0205 - val_acc: 0.7300\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9512 - acc: 0.7660 - val_loss: 1.0196 - val_acc: 0.7310\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9508 - acc: 0.7669 - val_loss: 1.0169 - val_acc: 0.7320\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9501 - acc: 0.7688 - val_loss: 1.0219 - val_acc: 0.7270\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9493 - acc: 0.7680 - val_loss: 1.0162 - val_acc: 0.7360\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9487 - acc: 0.7679 - val_loss: 1.0144 - val_acc: 0.7340\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9476 - acc: 0.7688 - val_loss: 1.0130 - val_acc: 0.7370\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9472 - acc: 0.7676 - val_loss: 1.0156 - val_acc: 0.7360\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9465 - acc: 0.7677 - val_loss: 1.0176 - val_acc: 0.7340\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9466 - acc: 0.7691 - val_loss: 1.0162 - val_acc: 0.7320\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9455 - acc: 0.7695 - val_loss: 1.0191 - val_acc: 0.7330\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9445 - acc: 0.7691 - val_loss: 1.0129 - val_acc: 0.7430\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9442 - acc: 0.7676 - val_loss: 1.0196 - val_acc: 0.7370\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9448 - acc: 0.7683 - val_loss: 1.0139 - val_acc: 0.7340\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9431 - acc: 0.7695 - val_loss: 1.0126 - val_acc: 0.7440\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9430 - acc: 0.7689 - val_loss: 1.0130 - val_acc: 0.7390\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9422 - acc: 0.7701 - val_loss: 1.0083 - val_acc: 0.7300\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9408 - acc: 0.7720 - val_loss: 1.0106 - val_acc: 0.7290\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9413 - acc: 0.7693 - val_loss: 1.0078 - val_acc: 0.7340\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9395 - acc: 0.7699 - val_loss: 1.0079 - val_acc: 0.7330\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9389 - acc: 0.7704 - val_loss: 1.0054 - val_acc: 0.7330\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9390 - acc: 0.7707 - val_loss: 1.0055 - val_acc: 0.7400\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9382 - acc: 0.7689 - val_loss: 1.0128 - val_acc: 0.7310\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9379 - acc: 0.7703 - val_loss: 1.0054 - val_acc: 0.7370\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9371 - acc: 0.7701 - val_loss: 1.0052 - val_acc: 0.7310\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9360 - acc: 0.7705 - val_loss: 1.0048 - val_acc: 0.7340\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9352 - acc: 0.7704 - val_loss: 1.0064 - val_acc: 0.7330\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9345 - acc: 0.7709 - val_loss: 1.0095 - val_acc: 0.7350\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9340 - acc: 0.7728 - val_loss: 1.0041 - val_acc: 0.7310\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9339 - acc: 0.7724 - val_loss: 1.0049 - val_acc: 0.7330\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9330 - acc: 0.7727 - val_loss: 1.0065 - val_acc: 0.7350\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9325 - acc: 0.7727 - val_loss: 1.0023 - val_acc: 0.7440\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9320 - acc: 0.7725 - val_loss: 1.0027 - val_acc: 0.7310\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9314 - acc: 0.7731 - val_loss: 1.0017 - val_acc: 0.7310\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9308 - acc: 0.7709 - val_loss: 1.0003 - val_acc: 0.7440\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9301 - acc: 0.7735 - val_loss: 1.0093 - val_acc: 0.7310\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9297 - acc: 0.7731 - val_loss: 1.0060 - val_acc: 0.7390\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9294 - acc: 0.7709 - val_loss: 1.0007 - val_acc: 0.7360\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9288 - acc: 0.7719 - val_loss: 0.9961 - val_acc: 0.7400\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9275 - acc: 0.7716 - val_loss: 1.0004 - val_acc: 0.7380\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9278 - acc: 0.7719 - val_loss: 0.9989 - val_acc: 0.7330\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9264 - acc: 0.7728 - val_loss: 0.9984 - val_acc: 0.7380\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9260 - acc: 0.7733 - val_loss: 1.0073 - val_acc: 0.7330\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9256 - acc: 0.7739 - val_loss: 0.9991 - val_acc: 0.7350\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9246 - acc: 0.7757 - val_loss: 0.9978 - val_acc: 0.7350\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9251 - acc: 0.7725 - val_loss: 1.0004 - val_acc: 0.7320\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9240 - acc: 0.7735 - val_loss: 1.0042 - val_acc: 0.7300\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9235 - acc: 0.7728 - val_loss: 0.9954 - val_acc: 0.7450\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9236 - acc: 0.7721 - val_loss: 0.9972 - val_acc: 0.7310\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9230 - acc: 0.7735 - val_loss: 0.9929 - val_acc: 0.7450\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9220 - acc: 0.7732 - val_loss: 1.0019 - val_acc: 0.7360\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9215 - acc: 0.7720 - val_loss: 1.0009 - val_acc: 0.7330\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9214 - acc: 0.7741 - val_loss: 0.9966 - val_acc: 0.7360\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9207 - acc: 0.7736 - val_loss: 0.9928 - val_acc: 0.7370\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9198 - acc: 0.7737 - val_loss: 0.9923 - val_acc: 0.7380\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9193 - acc: 0.7743 - val_loss: 0.9933 - val_acc: 0.7330\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9194 - acc: 0.7735 - val_loss: 0.9899 - val_acc: 0.7330\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9183 - acc: 0.7751 - val_loss: 0.9933 - val_acc: 0.7380\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9181 - acc: 0.7736 - val_loss: 0.9954 - val_acc: 0.7370\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9173 - acc: 0.7769 - val_loss: 0.9907 - val_acc: 0.7370\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9168 - acc: 0.7751 - val_loss: 0.9927 - val_acc: 0.7370\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9160 - acc: 0.7756 - val_loss: 0.9944 - val_acc: 0.7350\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9161 - acc: 0.7760 - val_loss: 0.9939 - val_acc: 0.7360\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9156 - acc: 0.7740 - val_loss: 0.9972 - val_acc: 0.7310\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9155 - acc: 0.7752 - val_loss: 0.9945 - val_acc: 0.7310\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9145 - acc: 0.7767 - val_loss: 0.9867 - val_acc: 0.7380\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9142 - acc: 0.7759 - val_loss: 0.9891 - val_acc: 0.7390\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9136 - acc: 0.7739 - val_loss: 0.9889 - val_acc: 0.7320\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9133 - acc: 0.7757 - val_loss: 0.9866 - val_acc: 0.7410\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9129 - acc: 0.7752 - val_loss: 0.9896 - val_acc: 0.7370\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9120 - acc: 0.7740 - val_loss: 0.9876 - val_acc: 0.7360\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9115 - acc: 0.7757 - val_loss: 0.9850 - val_acc: 0.7400\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9110 - acc: 0.7753 - val_loss: 0.9966 - val_acc: 0.7320\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9108 - acc: 0.7768 - val_loss: 0.9864 - val_acc: 0.7450\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9099 - acc: 0.7752 - val_loss: 0.9922 - val_acc: 0.7360\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9102 - acc: 0.7760 - val_loss: 0.9924 - val_acc: 0.7350\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9097 - acc: 0.7753 - val_loss: 0.9886 - val_acc: 0.7440\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9086 - acc: 0.7759 - val_loss: 0.9835 - val_acc: 0.7430\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9080 - acc: 0.7760 - val_loss: 0.9903 - val_acc: 0.7400\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9078 - acc: 0.7760 - val_loss: 0.9847 - val_acc: 0.7360\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9074 - acc: 0.7753 - val_loss: 0.9931 - val_acc: 0.7300\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9073 - acc: 0.7763 - val_loss: 0.9818 - val_acc: 0.7330\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9059 - acc: 0.7760 - val_loss: 1.0030 - val_acc: 0.7300\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9064 - acc: 0.7764 - val_loss: 0.9852 - val_acc: 0.7370\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9064 - acc: 0.7748 - val_loss: 0.9832 - val_acc: 0.7420\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9055 - acc: 0.7784 - val_loss: 0.9841 - val_acc: 0.7310\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9054 - acc: 0.7773 - val_loss: 0.9924 - val_acc: 0.7320\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9042 - acc: 0.7779 - val_loss: 0.9824 - val_acc: 0.7320\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9034 - acc: 0.7775 - val_loss: 0.9808 - val_acc: 0.7340\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9038 - acc: 0.7765 - val_loss: 0.9822 - val_acc: 0.7380\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9031 - acc: 0.7779 - val_loss: 0.9814 - val_acc: 0.7340\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9027 - acc: 0.7755 - val_loss: 0.9810 - val_acc: 0.7360\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9021 - acc: 0.7765 - val_loss: 0.9831 - val_acc: 0.7330\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9010 - acc: 0.7793 - val_loss: 0.9783 - val_acc: 0.7420\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9013 - acc: 0.7787 - val_loss: 0.9803 - val_acc: 0.7350\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9011 - acc: 0.7785 - val_loss: 0.9794 - val_acc: 0.7430\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9005 - acc: 0.7779 - val_loss: 0.9775 - val_acc: 0.7450\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8994 - acc: 0.7776 - val_loss: 0.9754 - val_acc: 0.7350\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8990 - acc: 0.7781 - val_loss: 0.9875 - val_acc: 0.7390\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8991 - acc: 0.7787 - val_loss: 0.9776 - val_acc: 0.7420\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8991 - acc: 0.7788 - val_loss: 0.9836 - val_acc: 0.7330\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8979 - acc: 0.7784 - val_loss: 0.9768 - val_acc: 0.7330\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8974 - acc: 0.7801 - val_loss: 0.9779 - val_acc: 0.7450\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8980 - acc: 0.7784 - val_loss: 0.9757 - val_acc: 0.7380\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8968 - acc: 0.7787 - val_loss: 0.9794 - val_acc: 0.7330\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8965 - acc: 0.7765 - val_loss: 0.9861 - val_acc: 0.7380\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8965 - acc: 0.7776 - val_loss: 0.9743 - val_acc: 0.7380\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8962 - acc: 0.7788 - val_loss: 0.9787 - val_acc: 0.7400\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8962 - acc: 0.7793 - val_loss: 0.9826 - val_acc: 0.7340\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8947 - acc: 0.7812 - val_loss: 0.9733 - val_acc: 0.7400\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8946 - acc: 0.7792 - val_loss: 0.9913 - val_acc: 0.7330\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8953 - acc: 0.7799 - val_loss: 0.9796 - val_acc: 0.7350\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8937 - acc: 0.7777 - val_loss: 0.9763 - val_acc: 0.7340\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8932 - acc: 0.7791 - val_loss: 0.9719 - val_acc: 0.7360\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8930 - acc: 0.7792 - val_loss: 0.9833 - val_acc: 0.7360\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8928 - acc: 0.7789 - val_loss: 0.9721 - val_acc: 0.7430\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8922 - acc: 0.7777 - val_loss: 0.9738 - val_acc: 0.7370\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8924 - acc: 0.7785 - val_loss: 0.9707 - val_acc: 0.7330\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8919 - acc: 0.7793 - val_loss: 0.9732 - val_acc: 0.7340\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8912 - acc: 0.7811 - val_loss: 0.9719 - val_acc: 0.7340\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8908 - acc: 0.7793 - val_loss: 0.9730 - val_acc: 0.7420\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8904 - acc: 0.7807 - val_loss: 0.9771 - val_acc: 0.7300\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8893 - acc: 0.7804 - val_loss: 0.9745 - val_acc: 0.7320\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8902 - acc: 0.7819 - val_loss: 0.9709 - val_acc: 0.7400\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8896 - acc: 0.7795 - val_loss: 0.9774 - val_acc: 0.7310\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8881 - acc: 0.7804 - val_loss: 0.9759 - val_acc: 0.7340\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8891 - acc: 0.7808 - val_loss: 0.9713 - val_acc: 0.7370\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8886 - acc: 0.7795 - val_loss: 0.9698 - val_acc: 0.7420\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8884 - acc: 0.7795 - val_loss: 0.9687 - val_acc: 0.7340\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8868 - acc: 0.7800 - val_loss: 0.9822 - val_acc: 0.7280\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8868 - acc: 0.7817 - val_loss: 0.9710 - val_acc: 0.7390\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8866 - acc: 0.7809 - val_loss: 0.9723 - val_acc: 0.7450\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8868 - acc: 0.7809 - val_loss: 0.9692 - val_acc: 0.7370\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8857 - acc: 0.7812 - val_loss: 0.9706 - val_acc: 0.7280\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8851 - acc: 0.7812 - val_loss: 0.9677 - val_acc: 0.7360\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8854 - acc: 0.7812 - val_loss: 0.9779 - val_acc: 0.7450\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8851 - acc: 0.7825 - val_loss: 0.9710 - val_acc: 0.7410\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8853 - acc: 0.7799 - val_loss: 0.9716 - val_acc: 0.7340\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8842 - acc: 0.7811 - val_loss: 0.9726 - val_acc: 0.7380\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8839 - acc: 0.7799 - val_loss: 0.9683 - val_acc: 0.7370\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8835 - acc: 0.7827 - val_loss: 0.9680 - val_acc: 0.7390\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8832 - acc: 0.7805 - val_loss: 0.9674 - val_acc: 0.7440\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8826 - acc: 0.7812 - val_loss: 0.9724 - val_acc: 0.7370\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8826 - acc: 0.7804 - val_loss: 0.9669 - val_acc: 0.7410\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8819 - acc: 0.7809 - val_loss: 0.9673 - val_acc: 0.7380\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8821 - acc: 0.7828 - val_loss: 0.9637 - val_acc: 0.7420\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8814 - acc: 0.7817 - val_loss: 0.9706 - val_acc: 0.7330\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8811 - acc: 0.7812 - val_loss: 0.9776 - val_acc: 0.7330\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8810 - acc: 0.7815 - val_loss: 0.9643 - val_acc: 0.7330\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8794 - acc: 0.7839 - val_loss: 0.9646 - val_acc: 0.7470\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8810 - acc: 0.7817 - val_loss: 0.9735 - val_acc: 0.7390\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8796 - acc: 0.7832 - val_loss: 0.9786 - val_acc: 0.7280\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8807 - acc: 0.7837 - val_loss: 0.9695 - val_acc: 0.7350\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8794 - acc: 0.7823 - val_loss: 0.9730 - val_acc: 0.7370\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8790 - acc: 0.7832 - val_loss: 0.9649 - val_acc: 0.7440\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8786 - acc: 0.7819 - val_loss: 0.9665 - val_acc: 0.7410\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8776 - acc: 0.7809 - val_loss: 0.9828 - val_acc: 0.7360\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8778 - acc: 0.7817 - val_loss: 0.9664 - val_acc: 0.7370\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8774 - acc: 0.7817 - val_loss: 0.9625 - val_acc: 0.7430\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8769 - acc: 0.7801 - val_loss: 0.9672 - val_acc: 0.7350\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8763 - acc: 0.7839 - val_loss: 0.9751 - val_acc: 0.7290\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8770 - acc: 0.7817 - val_loss: 0.9643 - val_acc: 0.7340\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8756 - acc: 0.7812 - val_loss: 0.9637 - val_acc: 0.7400\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8748 - acc: 0.7821 - val_loss: 0.9617 - val_acc: 0.7370\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8754 - acc: 0.7817 - val_loss: 0.9675 - val_acc: 0.7390\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8761 - acc: 0.7819 - val_loss: 0.9633 - val_acc: 0.7360\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8749 - acc: 0.7831 - val_loss: 0.9604 - val_acc: 0.7410\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8738 - acc: 0.7824 - val_loss: 0.9648 - val_acc: 0.7410\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8743 - acc: 0.7835 - val_loss: 0.9648 - val_acc: 0.7360\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8740 - acc: 0.7833 - val_loss: 0.9633 - val_acc: 0.7410\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8736 - acc: 0.7840 - val_loss: 0.9672 - val_acc: 0.7450\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8730 - acc: 0.7823 - val_loss: 0.9652 - val_acc: 0.7400\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8728 - acc: 0.7852 - val_loss: 0.9613 - val_acc: 0.7440\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8724 - acc: 0.7853 - val_loss: 0.9733 - val_acc: 0.7330\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8723 - acc: 0.7847 - val_loss: 0.9714 - val_acc: 0.7340\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8725 - acc: 0.7833 - val_loss: 0.9604 - val_acc: 0.7390\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8712 - acc: 0.7827 - val_loss: 0.9628 - val_acc: 0.7380\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8711 - acc: 0.7835 - val_loss: 0.9542 - val_acc: 0.7360\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8700 - acc: 0.7832 - val_loss: 0.9694 - val_acc: 0.7320\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8710 - acc: 0.7847 - val_loss: 0.9669 - val_acc: 0.7400\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8704 - acc: 0.7841 - val_loss: 0.9640 - val_acc: 0.7450\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8697 - acc: 0.7855 - val_loss: 0.9583 - val_acc: 0.7390\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8696 - acc: 0.7848 - val_loss: 0.9595 - val_acc: 0.7400\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8697 - acc: 0.7833 - val_loss: 0.9594 - val_acc: 0.7470\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8695 - acc: 0.7829 - val_loss: 0.9547 - val_acc: 0.7430\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8685 - acc: 0.7845 - val_loss: 0.9724 - val_acc: 0.7320\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8690 - acc: 0.7851 - val_loss: 0.9556 - val_acc: 0.7460\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8684 - acc: 0.7827 - val_loss: 0.9587 - val_acc: 0.7440\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8676 - acc: 0.7841 - val_loss: 0.9547 - val_acc: 0.7430\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8669 - acc: 0.7840 - val_loss: 0.9757 - val_acc: 0.7280\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8675 - acc: 0.7841 - val_loss: 0.9563 - val_acc: 0.7400\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8674 - acc: 0.7851 - val_loss: 0.9537 - val_acc: 0.7450\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8668 - acc: 0.7849 - val_loss: 0.9606 - val_acc: 0.7440\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8668 - acc: 0.7852 - val_loss: 0.9613 - val_acc: 0.7440\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8658 - acc: 0.7845 - val_loss: 0.9556 - val_acc: 0.7390\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8664 - acc: 0.7847 - val_loss: 0.9644 - val_acc: 0.7310\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8658 - acc: 0.7833 - val_loss: 0.9550 - val_acc: 0.7490\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8653 - acc: 0.7853 - val_loss: 0.9645 - val_acc: 0.7390\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8650 - acc: 0.7843 - val_loss: 0.9570 - val_acc: 0.7420\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8644 - acc: 0.7860 - val_loss: 0.9744 - val_acc: 0.7440\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8641 - acc: 0.7835 - val_loss: 0.9555 - val_acc: 0.7400\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8627 - acc: 0.7839 - val_loss: 0.9566 - val_acc: 0.7410\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8654 - acc: 0.7855 - val_loss: 0.9553 - val_acc: 0.7430\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8636 - acc: 0.7861 - val_loss: 0.9548 - val_acc: 0.7400\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8630 - acc: 0.7845 - val_loss: 0.9528 - val_acc: 0.7360\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8633 - acc: 0.7840 - val_loss: 0.9608 - val_acc: 0.7420\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8623 - acc: 0.7851 - val_loss: 0.9762 - val_acc: 0.7320\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8624 - acc: 0.7849 - val_loss: 0.9515 - val_acc: 0.7370\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8622 - acc: 0.7860 - val_loss: 0.9590 - val_acc: 0.7470\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8624 - acc: 0.7836 - val_loss: 0.9562 - val_acc: 0.7360\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8631 - acc: 0.7855 - val_loss: 0.9538 - val_acc: 0.7420\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8603 - acc: 0.7852 - val_loss: 0.9525 - val_acc: 0.7470\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8606 - acc: 0.7867 - val_loss: 0.9545 - val_acc: 0.7370\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8603 - acc: 0.7860 - val_loss: 0.9572 - val_acc: 0.7380\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8605 - acc: 0.7851 - val_loss: 0.9588 - val_acc: 0.7390\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8608 - acc: 0.7840 - val_loss: 0.9675 - val_acc: 0.7340\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8602 - acc: 0.7853 - val_loss: 0.9515 - val_acc: 0.7440\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8593 - acc: 0.7861 - val_loss: 0.9529 - val_acc: 0.7390\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8593 - acc: 0.7851 - val_loss: 0.9567 - val_acc: 0.7540\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8591 - acc: 0.7852 - val_loss: 0.9564 - val_acc: 0.7390\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8589 - acc: 0.7872 - val_loss: 0.9602 - val_acc: 0.7490\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8588 - acc: 0.7883 - val_loss: 0.9524 - val_acc: 0.7480\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8590 - acc: 0.7887 - val_loss: 0.9508 - val_acc: 0.7400\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8581 - acc: 0.7844 - val_loss: 0.9517 - val_acc: 0.7360\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8579 - acc: 0.7867 - val_loss: 0.9527 - val_acc: 0.7540\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8572 - acc: 0.7883 - val_loss: 0.9548 - val_acc: 0.7480\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8583 - acc: 0.7884 - val_loss: 0.9543 - val_acc: 0.7340\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8578 - acc: 0.7867 - val_loss: 0.9518 - val_acc: 0.7400\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8575 - acc: 0.7867 - val_loss: 0.9643 - val_acc: 0.7320\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8563 - acc: 0.7863 - val_loss: 0.9525 - val_acc: 0.7350\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8564 - acc: 0.7884 - val_loss: 0.9508 - val_acc: 0.7450\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8568 - acc: 0.7839 - val_loss: 0.9624 - val_acc: 0.7350\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8555 - acc: 0.7835 - val_loss: 0.9722 - val_acc: 0.7230\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8566 - acc: 0.7824 - val_loss: 0.9519 - val_acc: 0.7350\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8553 - acc: 0.7875 - val_loss: 0.9569 - val_acc: 0.7520\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8567 - acc: 0.7859 - val_loss: 0.9646 - val_acc: 0.7460\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8559 - acc: 0.7857 - val_loss: 0.9553 - val_acc: 0.7370\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8540 - acc: 0.7872 - val_loss: 0.9505 - val_acc: 0.7420\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8544 - acc: 0.7883 - val_loss: 0.9534 - val_acc: 0.7380\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8547 - acc: 0.7872 - val_loss: 0.9647 - val_acc: 0.7270\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8540 - acc: 0.7871 - val_loss: 0.9478 - val_acc: 0.7390\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8527 - acc: 0.7883 - val_loss: 0.9597 - val_acc: 0.7300\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8525 - acc: 0.7875 - val_loss: 0.9592 - val_acc: 0.7370\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8522 - acc: 0.7861 - val_loss: 0.9505 - val_acc: 0.7400\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8523 - acc: 0.7879 - val_loss: 0.9481 - val_acc: 0.7380\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8515 - acc: 0.7881 - val_loss: 0.9634 - val_acc: 0.7310\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8529 - acc: 0.7856 - val_loss: 0.9532 - val_acc: 0.7400\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8528 - acc: 0.7880 - val_loss: 0.9660 - val_acc: 0.7250\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8521 - acc: 0.7876 - val_loss: 0.9510 - val_acc: 0.7320\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8524 - acc: 0.7884 - val_loss: 0.9477 - val_acc: 0.7460\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8507 - acc: 0.7884 - val_loss: 0.9491 - val_acc: 0.7430\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8511 - acc: 0.7904 - val_loss: 0.9514 - val_acc: 0.7450\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8502 - acc: 0.7889 - val_loss: 0.9588 - val_acc: 0.7390\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8515 - acc: 0.7887 - val_loss: 0.9511 - val_acc: 0.7350\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8498 - acc: 0.7903 - val_loss: 0.9512 - val_acc: 0.7450\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8517 - acc: 0.7876 - val_loss: 0.9601 - val_acc: 0.7350\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8496 - acc: 0.7896 - val_loss: 0.9451 - val_acc: 0.7440\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8497 - acc: 0.7901 - val_loss: 0.9418 - val_acc: 0.7430\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8495 - acc: 0.7889 - val_loss: 0.9487 - val_acc: 0.7510\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8499 - acc: 0.7884 - val_loss: 0.9504 - val_acc: 0.7500\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8492 - acc: 0.7871 - val_loss: 0.9462 - val_acc: 0.7430\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8478 - acc: 0.7876 - val_loss: 0.9527 - val_acc: 0.7430\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8491 - acc: 0.7872 - val_loss: 0.9516 - val_acc: 0.7290\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8480 - acc: 0.7896 - val_loss: 0.9494 - val_acc: 0.7370\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8476 - acc: 0.7919 - val_loss: 0.9452 - val_acc: 0.7380\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8476 - acc: 0.7891 - val_loss: 0.9443 - val_acc: 0.7370\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 0.8474 - acc: 0.7876 - val_loss: 0.9501 - val_acc: 0.7400\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.8479 - acc: 0.7919 - val_loss: 0.9515 - val_acc: 0.7490\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8473 - acc: 0.7897 - val_loss: 0.9489 - val_acc: 0.7380\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8462 - acc: 0.7883 - val_loss: 0.9491 - val_acc: 0.7420\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8465 - acc: 0.7884 - val_loss: 0.9454 - val_acc: 0.7390\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8470 - acc: 0.7888 - val_loss: 0.9521 - val_acc: 0.7350\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8469 - acc: 0.7888 - val_loss: 0.9407 - val_acc: 0.7470\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8458 - acc: 0.7876 - val_loss: 0.9426 - val_acc: 0.7350\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8454 - acc: 0.7899 - val_loss: 0.9404 - val_acc: 0.7460\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8451 - acc: 0.7887 - val_loss: 0.9416 - val_acc: 0.7350\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8445 - acc: 0.7895 - val_loss: 0.9473 - val_acc: 0.7430\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8454 - acc: 0.7889 - val_loss: 0.9619 - val_acc: 0.7260\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8460 - acc: 0.7904 - val_loss: 0.9424 - val_acc: 0.7420\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8439 - acc: 0.7900 - val_loss: 0.9458 - val_acc: 0.7370\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8443 - acc: 0.7912 - val_loss: 0.9418 - val_acc: 0.7480\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8443 - acc: 0.7912 - val_loss: 0.9402 - val_acc: 0.7400\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8439 - acc: 0.7904 - val_loss: 0.9561 - val_acc: 0.7410\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8441 - acc: 0.7909 - val_loss: 0.9441 - val_acc: 0.7400\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8440 - acc: 0.7912 - val_loss: 0.9456 - val_acc: 0.7450\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8434 - acc: 0.7905 - val_loss: 0.9409 - val_acc: 0.7450\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8431 - acc: 0.7912 - val_loss: 0.9398 - val_acc: 0.7370\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8428 - acc: 0.7907 - val_loss: 0.9478 - val_acc: 0.7390\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8437 - acc: 0.7881 - val_loss: 0.9453 - val_acc: 0.7390\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8426 - acc: 0.7905 - val_loss: 0.9434 - val_acc: 0.7430\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8434 - acc: 0.7895 - val_loss: 0.9471 - val_acc: 0.7390\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8431 - acc: 0.7880 - val_loss: 0.9415 - val_acc: 0.7430\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8419 - acc: 0.7900 - val_loss: 0.9535 - val_acc: 0.7370\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8418 - acc: 0.7912 - val_loss: 0.9443 - val_acc: 0.7350\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8424 - acc: 0.7900 - val_loss: 0.9414 - val_acc: 0.7490\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8420 - acc: 0.7901 - val_loss: 0.9418 - val_acc: 0.7420\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8423 - acc: 0.7889 - val_loss: 0.9462 - val_acc: 0.7350\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8417 - acc: 0.7913 - val_loss: 0.9704 - val_acc: 0.7280\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8424 - acc: 0.7905 - val_loss: 0.9560 - val_acc: 0.7480\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8415 - acc: 0.7925 - val_loss: 0.9460 - val_acc: 0.7400\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8413 - acc: 0.7919 - val_loss: 0.9551 - val_acc: 0.7290\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8408 - acc: 0.7920 - val_loss: 0.9575 - val_acc: 0.7430\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8417 - acc: 0.7899 - val_loss: 0.9383 - val_acc: 0.7430\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8409 - acc: 0.7919 - val_loss: 0.9504 - val_acc: 0.7360\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8400 - acc: 0.7916 - val_loss: 0.9445 - val_acc: 0.7370\n",
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8403 - acc: 0.7913 - val_loss: 0.9509 - val_acc: 0.7370\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8386 - acc: 0.7941 - val_loss: 0.9429 - val_acc: 0.7400\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8404 - acc: 0.7931 - val_loss: 0.9421 - val_acc: 0.7460\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8389 - acc: 0.7915 - val_loss: 0.9460 - val_acc: 0.7350\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8384 - acc: 0.7940 - val_loss: 0.9487 - val_acc: 0.7340\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8395 - acc: 0.7933 - val_loss: 0.9388 - val_acc: 0.7480\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8384 - acc: 0.7931 - val_loss: 0.9445 - val_acc: 0.7510\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8387 - acc: 0.7933 - val_loss: 0.9391 - val_acc: 0.7430\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8385 - acc: 0.7905 - val_loss: 0.9403 - val_acc: 0.7420\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8376 - acc: 0.7923 - val_loss: 0.9461 - val_acc: 0.7480\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8375 - acc: 0.7920 - val_loss: 0.9451 - val_acc: 0.7350\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8382 - acc: 0.7917 - val_loss: 0.9411 - val_acc: 0.7520\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8370 - acc: 0.7924 - val_loss: 0.9871 - val_acc: 0.7170\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8373 - acc: 0.7933 - val_loss: 0.9434 - val_acc: 0.7440\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8365 - acc: 0.7927 - val_loss: 0.9404 - val_acc: 0.7430\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8367 - acc: 0.7901 - val_loss: 0.9379 - val_acc: 0.7430\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8361 - acc: 0.7919 - val_loss: 0.9496 - val_acc: 0.7320\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8364 - acc: 0.7939 - val_loss: 0.9472 - val_acc: 0.7460\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8369 - acc: 0.7923 - val_loss: 0.9475 - val_acc: 0.7320\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8359 - acc: 0.7953 - val_loss: 0.9445 - val_acc: 0.7430\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8352 - acc: 0.7931 - val_loss: 0.9376 - val_acc: 0.7430\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8366 - acc: 0.7931 - val_loss: 0.9399 - val_acc: 0.7490\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8354 - acc: 0.7941 - val_loss: 0.9371 - val_acc: 0.7440\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8355 - acc: 0.7945 - val_loss: 0.9387 - val_acc: 0.7370\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8356 - acc: 0.7936 - val_loss: 0.9479 - val_acc: 0.7390\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.8342 - acc: 0.792 - 0s 60us/step - loss: 0.8354 - acc: 0.7911 - val_loss: 0.9382 - val_acc: 0.7470\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8356 - acc: 0.7915 - val_loss: 0.9446 - val_acc: 0.7390\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8346 - acc: 0.7925 - val_loss: 0.9490 - val_acc: 0.7340\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8345 - acc: 0.7917 - val_loss: 0.9371 - val_acc: 0.7420\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8347 - acc: 0.7923 - val_loss: 0.9344 - val_acc: 0.7370\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8341 - acc: 0.7921 - val_loss: 0.9438 - val_acc: 0.7410\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8349 - acc: 0.7919 - val_loss: 0.9406 - val_acc: 0.7460\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8345 - acc: 0.7931 - val_loss: 0.9452 - val_acc: 0.7450\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8338 - acc: 0.7933 - val_loss: 0.9440 - val_acc: 0.7320\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8339 - acc: 0.7943 - val_loss: 0.9464 - val_acc: 0.7290\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8342 - acc: 0.7925 - val_loss: 0.9360 - val_acc: 0.7340\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8320 - acc: 0.7941 - val_loss: 0.9378 - val_acc: 0.7380\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8324 - acc: 0.7949 - val_loss: 0.9434 - val_acc: 0.7350\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8327 - acc: 0.7948 - val_loss: 0.9490 - val_acc: 0.7360\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8331 - acc: 0.7952 - val_loss: 0.9342 - val_acc: 0.7460\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8327 - acc: 0.7940 - val_loss: 0.9467 - val_acc: 0.7390\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8321 - acc: 0.7940 - val_loss: 0.9458 - val_acc: 0.7350\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8331 - acc: 0.7924 - val_loss: 0.9429 - val_acc: 0.7470\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8319 - acc: 0.7933 - val_loss: 0.9420 - val_acc: 0.7560\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8318 - acc: 0.7940 - val_loss: 0.9449 - val_acc: 0.7480\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8314 - acc: 0.7955 - val_loss: 0.9477 - val_acc: 0.7360\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8329 - acc: 0.7939 - val_loss: 0.9416 - val_acc: 0.7500\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8308 - acc: 0.7941 - val_loss: 0.9423 - val_acc: 0.7420\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8318 - acc: 0.7935 - val_loss: 0.9350 - val_acc: 0.7480\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8307 - acc: 0.7948 - val_loss: 0.9369 - val_acc: 0.7410\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8300 - acc: 0.7940 - val_loss: 0.9564 - val_acc: 0.7400\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8317 - acc: 0.7928 - val_loss: 0.9449 - val_acc: 0.7390\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8311 - acc: 0.7933 - val_loss: 0.9447 - val_acc: 0.7410\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8301 - acc: 0.7933 - val_loss: 0.9376 - val_acc: 0.7390\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8294 - acc: 0.7957 - val_loss: 0.9419 - val_acc: 0.7440\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8294 - acc: 0.7976 - val_loss: 0.9377 - val_acc: 0.7420\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8305 - acc: 0.7937 - val_loss: 0.9503 - val_acc: 0.7390\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8324 - acc: 0.7949 - val_loss: 0.9559 - val_acc: 0.7390\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8290 - acc: 0.7964 - val_loss: 0.9357 - val_acc: 0.7380\n",
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8290 - acc: 0.7961 - val_loss: 0.9415 - val_acc: 0.7440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 709/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8298 - acc: 0.7948 - val_loss: 0.9419 - val_acc: 0.7490\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8297 - acc: 0.7959 - val_loss: 0.9445 - val_acc: 0.7480\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8299 - acc: 0.7948 - val_loss: 0.9319 - val_acc: 0.7460\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8280 - acc: 0.7951 - val_loss: 0.9500 - val_acc: 0.7300\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8296 - acc: 0.7945 - val_loss: 0.9446 - val_acc: 0.7430\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8287 - acc: 0.7941 - val_loss: 0.9357 - val_acc: 0.7480\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8294 - acc: 0.7959 - val_loss: 0.9554 - val_acc: 0.7300\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8284 - acc: 0.7965 - val_loss: 0.9363 - val_acc: 0.7440\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8274 - acc: 0.7969 - val_loss: 0.9436 - val_acc: 0.7370\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8271 - acc: 0.7967 - val_loss: 0.9449 - val_acc: 0.7380\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8283 - acc: 0.7956 - val_loss: 0.9389 - val_acc: 0.7380\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8276 - acc: 0.7953 - val_loss: 0.9374 - val_acc: 0.7440\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8268 - acc: 0.7948 - val_loss: 0.9527 - val_acc: 0.7400\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8281 - acc: 0.7959 - val_loss: 0.9784 - val_acc: 0.7360\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8288 - acc: 0.7947 - val_loss: 0.9378 - val_acc: 0.7430\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8268 - acc: 0.7969 - val_loss: 0.9398 - val_acc: 0.7410\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8259 - acc: 0.7973 - val_loss: 0.9367 - val_acc: 0.7490\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8271 - acc: 0.7963 - val_loss: 0.9437 - val_acc: 0.7340\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8263 - acc: 0.7968 - val_loss: 0.9420 - val_acc: 0.7520\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8266 - acc: 0.7964 - val_loss: 0.9336 - val_acc: 0.7360\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8257 - acc: 0.7936 - val_loss: 0.9336 - val_acc: 0.7410\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8254 - acc: 0.7968 - val_loss: 0.9312 - val_acc: 0.7430\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8257 - acc: 0.7936 - val_loss: 0.9471 - val_acc: 0.7400\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8249 - acc: 0.7968 - val_loss: 0.9356 - val_acc: 0.7460\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8251 - acc: 0.7967 - val_loss: 0.9395 - val_acc: 0.7360\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8261 - acc: 0.7957 - val_loss: 0.9555 - val_acc: 0.7420\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8258 - acc: 0.7995 - val_loss: 0.9428 - val_acc: 0.7440\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8251 - acc: 0.7953 - val_loss: 0.9615 - val_acc: 0.7360\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8250 - acc: 0.7952 - val_loss: 0.9420 - val_acc: 0.7420\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8247 - acc: 0.7979 - val_loss: 0.9371 - val_acc: 0.7450\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8239 - acc: 0.7955 - val_loss: 0.9352 - val_acc: 0.7360\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8249 - acc: 0.7960 - val_loss: 0.9341 - val_acc: 0.7420\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8246 - acc: 0.7936 - val_loss: 0.9346 - val_acc: 0.7440\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8246 - acc: 0.7948 - val_loss: 0.9373 - val_acc: 0.7400\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8238 - acc: 0.7972 - val_loss: 0.9470 - val_acc: 0.7400\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8239 - acc: 0.7959 - val_loss: 0.9340 - val_acc: 0.7420\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8232 - acc: 0.7952 - val_loss: 0.9310 - val_acc: 0.7430\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8225 - acc: 0.7951 - val_loss: 0.9357 - val_acc: 0.7460\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8225 - acc: 0.7953 - val_loss: 0.9346 - val_acc: 0.7370\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8253 - acc: 0.7923 - val_loss: 0.9420 - val_acc: 0.7460\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8227 - acc: 0.7979 - val_loss: 0.9336 - val_acc: 0.7420\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8246 - acc: 0.7956 - val_loss: 0.9422 - val_acc: 0.7450\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8216 - acc: 0.7969 - val_loss: 0.9384 - val_acc: 0.7390\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8214 - acc: 0.7991 - val_loss: 0.9381 - val_acc: 0.7390\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8234 - acc: 0.7947 - val_loss: 0.9474 - val_acc: 0.7380\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8238 - acc: 0.7975 - val_loss: 0.9459 - val_acc: 0.7480\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8222 - acc: 0.7975 - val_loss: 0.9398 - val_acc: 0.7420\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8220 - acc: 0.7981 - val_loss: 0.9565 - val_acc: 0.7490\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8228 - acc: 0.7951 - val_loss: 0.9386 - val_acc: 0.7410\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8226 - acc: 0.7960 - val_loss: 0.9321 - val_acc: 0.7440\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8209 - acc: 0.7972 - val_loss: 0.9389 - val_acc: 0.7450\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8222 - acc: 0.7983 - val_loss: 0.9332 - val_acc: 0.7420\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8206 - acc: 0.7975 - val_loss: 0.9470 - val_acc: 0.7420\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8218 - acc: 0.7961 - val_loss: 0.9405 - val_acc: 0.7450\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8225 - acc: 0.7975 - val_loss: 0.9401 - val_acc: 0.7380\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8210 - acc: 0.7995 - val_loss: 0.9382 - val_acc: 0.7450\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8195 - acc: 0.8003 - val_loss: 0.9397 - val_acc: 0.7440\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8214 - acc: 0.7991 - val_loss: 0.9337 - val_acc: 0.7460\n",
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8200 - acc: 0.8001 - val_loss: 0.9354 - val_acc: 0.7460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 768/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8199 - acc: 0.7981 - val_loss: 0.9434 - val_acc: 0.7490\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8207 - acc: 0.7959 - val_loss: 0.9299 - val_acc: 0.7470\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8179 - acc: 0.7997 - val_loss: 0.9362 - val_acc: 0.7460\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8189 - acc: 0.7987 - val_loss: 0.9421 - val_acc: 0.7440\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8193 - acc: 0.8005 - val_loss: 0.9369 - val_acc: 0.7450\n",
      "Epoch 773/1000\n",
      "4352/7500 [================>.............] - ETA: 0s - loss: 0.8198 - acc: 0.7983"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 2.0228 - acc: 0.1372 - val_loss: 1.9610 - val_acc: 0.1380\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9760 - acc: 0.1439 - val_loss: 1.9430 - val_acc: 0.1630\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9560 - acc: 0.1548 - val_loss: 1.9328 - val_acc: 0.1790\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9462 - acc: 0.1639 - val_loss: 1.9249 - val_acc: 0.1890\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9412 - acc: 0.1661 - val_loss: 1.9189 - val_acc: 0.2040\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9261 - acc: 0.1800 - val_loss: 1.9124 - val_acc: 0.2090\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9231 - acc: 0.1896 - val_loss: 1.9062 - val_acc: 0.2310\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9152 - acc: 0.1916 - val_loss: 1.8993 - val_acc: 0.2430\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9105 - acc: 0.1981 - val_loss: 1.8928 - val_acc: 0.2540\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9032 - acc: 0.2036 - val_loss: 1.8854 - val_acc: 0.2590\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8927 - acc: 0.2129 - val_loss: 1.8773 - val_acc: 0.2640\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8881 - acc: 0.2221 - val_loss: 1.8688 - val_acc: 0.2660\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8835 - acc: 0.2249 - val_loss: 1.8599 - val_acc: 0.2720\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8740 - acc: 0.2337 - val_loss: 1.8489 - val_acc: 0.2840\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8637 - acc: 0.2427 - val_loss: 1.8371 - val_acc: 0.2930\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8544 - acc: 0.2432 - val_loss: 1.8235 - val_acc: 0.3030\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8401 - acc: 0.2505 - val_loss: 1.8080 - val_acc: 0.3100\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8334 - acc: 0.2677 - val_loss: 1.7913 - val_acc: 0.3120\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8201 - acc: 0.2657 - val_loss: 1.7722 - val_acc: 0.3190\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8009 - acc: 0.2712 - val_loss: 1.7512 - val_acc: 0.3240\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7911 - acc: 0.2812 - val_loss: 1.7285 - val_acc: 0.3310\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7735 - acc: 0.2939 - val_loss: 1.7043 - val_acc: 0.3420\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7500 - acc: 0.3079 - val_loss: 1.6776 - val_acc: 0.3510\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7402 - acc: 0.3083 - val_loss: 1.6486 - val_acc: 0.3800\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7199 - acc: 0.3201 - val_loss: 1.6207 - val_acc: 0.3930\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6974 - acc: 0.3253 - val_loss: 1.5931 - val_acc: 0.4070\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6781 - acc: 0.3377 - val_loss: 1.5651 - val_acc: 0.4270\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6711 - acc: 0.3471 - val_loss: 1.5406 - val_acc: 0.4440\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6474 - acc: 0.3469 - val_loss: 1.5145 - val_acc: 0.4510\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6281 - acc: 0.3521 - val_loss: 1.4876 - val_acc: 0.4610\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6150 - acc: 0.3639 - val_loss: 1.4634 - val_acc: 0.4750\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5985 - acc: 0.3748 - val_loss: 1.4395 - val_acc: 0.4940\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5763 - acc: 0.3808 - val_loss: 1.4134 - val_acc: 0.5050\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5539 - acc: 0.3969 - val_loss: 1.3892 - val_acc: 0.5280\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5548 - acc: 0.3883 - val_loss: 1.3709 - val_acc: 0.5280\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5287 - acc: 0.4059 - val_loss: 1.3478 - val_acc: 0.5470\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5163 - acc: 0.4099 - val_loss: 1.3288 - val_acc: 0.5560\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5083 - acc: 0.4137 - val_loss: 1.3094 - val_acc: 0.5720\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4847 - acc: 0.4319 - val_loss: 1.2877 - val_acc: 0.5770\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4855 - acc: 0.4204 - val_loss: 1.2714 - val_acc: 0.5920\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4642 - acc: 0.4336 - val_loss: 1.2518 - val_acc: 0.6070\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4565 - acc: 0.4380 - val_loss: 1.2329 - val_acc: 0.6190\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4303 - acc: 0.4479 - val_loss: 1.2134 - val_acc: 0.6380\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4283 - acc: 0.4561 - val_loss: 1.1985 - val_acc: 0.6450\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3998 - acc: 0.4632 - val_loss: 1.1791 - val_acc: 0.6420\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3886 - acc: 0.4664 - val_loss: 1.1616 - val_acc: 0.6460\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3729 - acc: 0.4784 - val_loss: 1.1441 - val_acc: 0.6540\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3671 - acc: 0.4807 - val_loss: 1.1300 - val_acc: 0.6640\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3640 - acc: 0.4808 - val_loss: 1.1187 - val_acc: 0.6570\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3373 - acc: 0.4841 - val_loss: 1.0995 - val_acc: 0.6660\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3246 - acc: 0.4999 - val_loss: 1.0832 - val_acc: 0.6730\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3099 - acc: 0.5103 - val_loss: 1.0693 - val_acc: 0.6760\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3033 - acc: 0.5064 - val_loss: 1.0562 - val_acc: 0.6850\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2944 - acc: 0.5116 - val_loss: 1.0446 - val_acc: 0.6820\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2866 - acc: 0.5151 - val_loss: 1.0344 - val_acc: 0.6850\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2655 - acc: 0.5233 - val_loss: 1.0180 - val_acc: 0.6880\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2563 - acc: 0.5239 - val_loss: 1.0041 - val_acc: 0.6920\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2481 - acc: 0.5256 - val_loss: 0.9925 - val_acc: 0.6950\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2476 - acc: 0.5280 - val_loss: 0.9800 - val_acc: 0.6940\n",
      "Epoch 60/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2354 - acc: 0.5300 - val_loss: 0.9684 - val_acc: 0.7050\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2150 - acc: 0.5400 - val_loss: 0.9589 - val_acc: 0.7120\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2109 - acc: 0.5513 - val_loss: 0.9459 - val_acc: 0.7030\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2099 - acc: 0.5472 - val_loss: 0.9360 - val_acc: 0.7070\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1975 - acc: 0.5485 - val_loss: 0.9275 - val_acc: 0.7140\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1849 - acc: 0.5585 - val_loss: 0.9184 - val_acc: 0.7120\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1837 - acc: 0.5669 - val_loss: 0.9071 - val_acc: 0.7230\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1751 - acc: 0.5639 - val_loss: 0.8986 - val_acc: 0.7200\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1549 - acc: 0.5747 - val_loss: 0.8873 - val_acc: 0.7260\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1635 - acc: 0.5651 - val_loss: 0.8807 - val_acc: 0.7260\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1557 - acc: 0.5672 - val_loss: 0.8725 - val_acc: 0.7280\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1337 - acc: 0.5803 - val_loss: 0.8605 - val_acc: 0.7310\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1170 - acc: 0.5859 - val_loss: 0.8531 - val_acc: 0.7290\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1179 - acc: 0.5797 - val_loss: 0.8437 - val_acc: 0.7330\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1314 - acc: 0.5715 - val_loss: 0.8400 - val_acc: 0.7340\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0935 - acc: 0.5948 - val_loss: 0.8310 - val_acc: 0.7370\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0940 - acc: 0.5903 - val_loss: 0.8213 - val_acc: 0.7360\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0864 - acc: 0.5919 - val_loss: 0.8140 - val_acc: 0.7370\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0872 - acc: 0.5936 - val_loss: 0.8073 - val_acc: 0.7370\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0767 - acc: 0.5975 - val_loss: 0.7990 - val_acc: 0.7370\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0606 - acc: 0.6045 - val_loss: 0.7912 - val_acc: 0.7390\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0702 - acc: 0.6021 - val_loss: 0.7851 - val_acc: 0.7400\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0764 - acc: 0.5968 - val_loss: 0.7811 - val_acc: 0.7430\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0551 - acc: 0.6103 - val_loss: 0.7777 - val_acc: 0.7410\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0540 - acc: 0.6044 - val_loss: 0.7700 - val_acc: 0.7450\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0394 - acc: 0.6139 - val_loss: 0.7644 - val_acc: 0.7470\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0397 - acc: 0.6067 - val_loss: 0.7589 - val_acc: 0.7470\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0272 - acc: 0.6219 - val_loss: 0.7505 - val_acc: 0.7510\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0158 - acc: 0.6245 - val_loss: 0.7446 - val_acc: 0.7460\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0275 - acc: 0.6136 - val_loss: 0.7412 - val_acc: 0.7500\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0228 - acc: 0.6173 - val_loss: 0.7377 - val_acc: 0.7490\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0199 - acc: 0.6184 - val_loss: 0.7353 - val_acc: 0.7500\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0134 - acc: 0.6215 - val_loss: 0.7303 - val_acc: 0.7530\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0174 - acc: 0.6208 - val_loss: 0.7271 - val_acc: 0.7500\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0023 - acc: 0.6265 - val_loss: 0.7241 - val_acc: 0.7500\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0029 - acc: 0.6220 - val_loss: 0.7190 - val_acc: 0.7540\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9879 - acc: 0.6327 - val_loss: 0.7127 - val_acc: 0.7560\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9865 - acc: 0.6252 - val_loss: 0.7100 - val_acc: 0.7550\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9628 - acc: 0.6372 - val_loss: 0.7027 - val_acc: 0.7590\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9743 - acc: 0.6404 - val_loss: 0.7010 - val_acc: 0.7580\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9698 - acc: 0.6383 - val_loss: 0.6961 - val_acc: 0.7580\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9758 - acc: 0.6328 - val_loss: 0.6926 - val_acc: 0.7590\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9662 - acc: 0.6391 - val_loss: 0.6881 - val_acc: 0.7580\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9672 - acc: 0.6409 - val_loss: 0.6854 - val_acc: 0.7550\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9480 - acc: 0.6501 - val_loss: 0.6818 - val_acc: 0.7640\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9476 - acc: 0.6475 - val_loss: 0.6776 - val_acc: 0.7610\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9526 - acc: 0.6485 - val_loss: 0.6744 - val_acc: 0.7600\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9478 - acc: 0.6456 - val_loss: 0.6734 - val_acc: 0.7620\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9373 - acc: 0.6532 - val_loss: 0.6690 - val_acc: 0.7600\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9404 - acc: 0.6469 - val_loss: 0.6653 - val_acc: 0.7620\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9366 - acc: 0.6524 - val_loss: 0.6629 - val_acc: 0.7620\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9141 - acc: 0.6569 - val_loss: 0.6577 - val_acc: 0.7650\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9396 - acc: 0.6484 - val_loss: 0.6591 - val_acc: 0.7620\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9192 - acc: 0.6569 - val_loss: 0.6553 - val_acc: 0.7630\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9250 - acc: 0.6605 - val_loss: 0.6542 - val_acc: 0.7620\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9224 - acc: 0.6533 - val_loss: 0.6496 - val_acc: 0.7640\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9200 - acc: 0.6568 - val_loss: 0.6510 - val_acc: 0.7640\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9003 - acc: 0.6680 - val_loss: 0.6470 - val_acc: 0.7610\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9010 - acc: 0.6671 - val_loss: 0.6434 - val_acc: 0.7660\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9034 - acc: 0.6628 - val_loss: 0.6428 - val_acc: 0.7640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9110 - acc: 0.6584 - val_loss: 0.6407 - val_acc: 0.7640\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8934 - acc: 0.6695 - val_loss: 0.6380 - val_acc: 0.7670\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8921 - acc: 0.6660 - val_loss: 0.6374 - val_acc: 0.7640\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8894 - acc: 0.6664 - val_loss: 0.6322 - val_acc: 0.7660\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8856 - acc: 0.6701 - val_loss: 0.6285 - val_acc: 0.7670\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8889 - acc: 0.6688 - val_loss: 0.6283 - val_acc: 0.7700\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8769 - acc: 0.6671 - val_loss: 0.6276 - val_acc: 0.7680\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8794 - acc: 0.6720 - val_loss: 0.6248 - val_acc: 0.7670\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8593 - acc: 0.6759 - val_loss: 0.6234 - val_acc: 0.7690\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8801 - acc: 0.6691 - val_loss: 0.6223 - val_acc: 0.7690\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8658 - acc: 0.6731 - val_loss: 0.6190 - val_acc: 0.7720\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8869 - acc: 0.6708 - val_loss: 0.6189 - val_acc: 0.7750\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8563 - acc: 0.6824 - val_loss: 0.6160 - val_acc: 0.7750\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8640 - acc: 0.6839 - val_loss: 0.6151 - val_acc: 0.7740\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8484 - acc: 0.6855 - val_loss: 0.6113 - val_acc: 0.7710\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8609 - acc: 0.6819 - val_loss: 0.6112 - val_acc: 0.7730\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8558 - acc: 0.6849 - val_loss: 0.6074 - val_acc: 0.7730\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8475 - acc: 0.6873 - val_loss: 0.6076 - val_acc: 0.7700\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8470 - acc: 0.6843 - val_loss: 0.6069 - val_acc: 0.7720\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8506 - acc: 0.6827 - val_loss: 0.6039 - val_acc: 0.7750\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8442 - acc: 0.6788 - val_loss: 0.6027 - val_acc: 0.7720\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8418 - acc: 0.6872 - val_loss: 0.6037 - val_acc: 0.7760\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8406 - acc: 0.6839 - val_loss: 0.6015 - val_acc: 0.7760\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8358 - acc: 0.6884 - val_loss: 0.5979 - val_acc: 0.7790\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8416 - acc: 0.6879 - val_loss: 0.5957 - val_acc: 0.7770\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8329 - acc: 0.6801 - val_loss: 0.5942 - val_acc: 0.7750\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8179 - acc: 0.6956 - val_loss: 0.5947 - val_acc: 0.7760\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8390 - acc: 0.6831 - val_loss: 0.5946 - val_acc: 0.7770\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8243 - acc: 0.6896 - val_loss: 0.5922 - val_acc: 0.7760\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8181 - acc: 0.6913 - val_loss: 0.5918 - val_acc: 0.7810\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8336 - acc: 0.6841 - val_loss: 0.5914 - val_acc: 0.7810\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8093 - acc: 0.6941 - val_loss: 0.5911 - val_acc: 0.7800\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8186 - acc: 0.6928 - val_loss: 0.5864 - val_acc: 0.7850\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8137 - acc: 0.7011 - val_loss: 0.5850 - val_acc: 0.7760\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8236 - acc: 0.6940 - val_loss: 0.5852 - val_acc: 0.7820\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8231 - acc: 0.6897 - val_loss: 0.5852 - val_acc: 0.7830\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8060 - acc: 0.7013 - val_loss: 0.5845 - val_acc: 0.7790\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7927 - acc: 0.7033 - val_loss: 0.5826 - val_acc: 0.7810\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7906 - acc: 0.7007 - val_loss: 0.5800 - val_acc: 0.7840\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8047 - acc: 0.6956 - val_loss: 0.5789 - val_acc: 0.7820\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8041 - acc: 0.7039 - val_loss: 0.5785 - val_acc: 0.7840\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7914 - acc: 0.7107 - val_loss: 0.5778 - val_acc: 0.7770\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7874 - acc: 0.7096 - val_loss: 0.5766 - val_acc: 0.7780\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7860 - acc: 0.7115 - val_loss: 0.5753 - val_acc: 0.7880\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7962 - acc: 0.7027 - val_loss: 0.5772 - val_acc: 0.7800\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7840 - acc: 0.7115 - val_loss: 0.5736 - val_acc: 0.7850\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7821 - acc: 0.7073 - val_loss: 0.5730 - val_acc: 0.7820\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7884 - acc: 0.7077 - val_loss: 0.5700 - val_acc: 0.7840\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7944 - acc: 0.7095 - val_loss: 0.5711 - val_acc: 0.7820\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7728 - acc: 0.7117 - val_loss: 0.5682 - val_acc: 0.7810\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7821 - acc: 0.7048 - val_loss: 0.5666 - val_acc: 0.7840\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7830 - acc: 0.7076 - val_loss: 0.5673 - val_acc: 0.7840\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7835 - acc: 0.7121 - val_loss: 0.5684 - val_acc: 0.7830\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7745 - acc: 0.7136 - val_loss: 0.5668 - val_acc: 0.7850\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7770 - acc: 0.7079 - val_loss: 0.5686 - val_acc: 0.7810\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7860 - acc: 0.7053 - val_loss: 0.5658 - val_acc: 0.7860\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7738 - acc: 0.7079 - val_loss: 0.5644 - val_acc: 0.7880\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7666 - acc: 0.7068 - val_loss: 0.5638 - val_acc: 0.7820\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7707 - acc: 0.7079 - val_loss: 0.5627 - val_acc: 0.7840\n",
      "Epoch 179/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7840 - acc: 0.7064 - val_loss: 0.5618 - val_acc: 0.7870\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7650 - acc: 0.7151 - val_loss: 0.5598 - val_acc: 0.7850\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7661 - acc: 0.7088 - val_loss: 0.5599 - val_acc: 0.7860\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7444 - acc: 0.7239 - val_loss: 0.5568 - val_acc: 0.7870\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7523 - acc: 0.7149 - val_loss: 0.5578 - val_acc: 0.7830\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7616 - acc: 0.7185 - val_loss: 0.5593 - val_acc: 0.7850\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7566 - acc: 0.7108 - val_loss: 0.5588 - val_acc: 0.7860\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7467 - acc: 0.7197 - val_loss: 0.5590 - val_acc: 0.7830\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7537 - acc: 0.7157 - val_loss: 0.5566 - val_acc: 0.7800\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7494 - acc: 0.7199 - val_loss: 0.5566 - val_acc: 0.7830\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7526 - acc: 0.7203 - val_loss: 0.5554 - val_acc: 0.7880\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7362 - acc: 0.7256 - val_loss: 0.5528 - val_acc: 0.7860\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7464 - acc: 0.7181 - val_loss: 0.5523 - val_acc: 0.7850\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7515 - acc: 0.7141 - val_loss: 0.5511 - val_acc: 0.7850\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7432 - acc: 0.7191 - val_loss: 0.5511 - val_acc: 0.7880\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7388 - acc: 0.7232 - val_loss: 0.5523 - val_acc: 0.7850\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7412 - acc: 0.7212 - val_loss: 0.5543 - val_acc: 0.7840\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7288 - acc: 0.7279 - val_loss: 0.5510 - val_acc: 0.7830\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7335 - acc: 0.7285 - val_loss: 0.5509 - val_acc: 0.7820\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7255 - acc: 0.7292 - val_loss: 0.5480 - val_acc: 0.7840\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7341 - acc: 0.7236 - val_loss: 0.5490 - val_acc: 0.7830\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7398 - acc: 0.7244 - val_loss: 0.5489 - val_acc: 0.7840\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step\n",
      "1500/1500 [==============================] - 0s 26us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.44953240927060445, 0.8355999999682109]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6567809325853984, 0.745333333492279]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 1.9131 - acc: 0.1977 - val_loss: 1.8734 - val_acc: 0.2517\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 1.8204 - acc: 0.3034 - val_loss: 1.7551 - val_acc: 0.3397\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 1.6686 - acc: 0.4072 - val_loss: 1.5741 - val_acc: 0.4647\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 1.4662 - acc: 0.5248 - val_loss: 1.3619 - val_acc: 0.5560\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 1.2557 - acc: 0.6060 - val_loss: 1.1666 - val_acc: 0.6303\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 1.0768 - acc: 0.6660 - val_loss: 1.0120 - val_acc: 0.6777\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.9451 - acc: 0.7012 - val_loss: 0.9037 - val_acc: 0.7047\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.8536 - acc: 0.7191 - val_loss: 0.8281 - val_acc: 0.7210\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7894 - acc: 0.7321 - val_loss: 0.7750 - val_acc: 0.7300\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7434 - acc: 0.7419 - val_loss: 0.7363 - val_acc: 0.7397\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7086 - acc: 0.7492 - val_loss: 0.7075 - val_acc: 0.7450\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6815 - acc: 0.7555 - val_loss: 0.6846 - val_acc: 0.7507\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6596 - acc: 0.7612 - val_loss: 0.6661 - val_acc: 0.7587\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6413 - acc: 0.7678 - val_loss: 0.6501 - val_acc: 0.7623\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.6259 - acc: 0.7724 - val_loss: 0.6383 - val_acc: 0.7653\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.6119 - acc: 0.7780 - val_loss: 0.6275 - val_acc: 0.7700\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5999 - acc: 0.7809 - val_loss: 0.6168 - val_acc: 0.7720\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5891 - acc: 0.7845 - val_loss: 0.6079 - val_acc: 0.7757\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5792 - acc: 0.7892 - val_loss: 0.6013 - val_acc: 0.7753\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5700 - acc: 0.7920 - val_loss: 0.5924 - val_acc: 0.7813\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5614 - acc: 0.7954 - val_loss: 0.5879 - val_acc: 0.7807\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5535 - acc: 0.7989 - val_loss: 0.5832 - val_acc: 0.7817\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5459 - acc: 0.8010 - val_loss: 0.5766 - val_acc: 0.7847\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5391 - acc: 0.8044 - val_loss: 0.5735 - val_acc: 0.7850\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5326 - acc: 0.8076 - val_loss: 0.5674 - val_acc: 0.7937\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5264 - acc: 0.8092 - val_loss: 0.5622 - val_acc: 0.7920\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5198 - acc: 0.8114 - val_loss: 0.5599 - val_acc: 0.7977\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5144 - acc: 0.8140 - val_loss: 0.5571 - val_acc: 0.8000\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5087 - acc: 0.8162 - val_loss: 0.5509 - val_acc: 0.8000\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5033 - acc: 0.8180 - val_loss: 0.5483 - val_acc: 0.8020\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4982 - acc: 0.8205 - val_loss: 0.5443 - val_acc: 0.8023\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4934 - acc: 0.8222 - val_loss: 0.5435 - val_acc: 0.8027\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4885 - acc: 0.8253 - val_loss: 0.5426 - val_acc: 0.8033\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4840 - acc: 0.8256 - val_loss: 0.5386 - val_acc: 0.8080\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4799 - acc: 0.8278 - val_loss: 0.5341 - val_acc: 0.8093\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4755 - acc: 0.8305 - val_loss: 0.5322 - val_acc: 0.8100\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4713 - acc: 0.8308 - val_loss: 0.5297 - val_acc: 0.8117\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4674 - acc: 0.8319 - val_loss: 0.5273 - val_acc: 0.8123\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4632 - acc: 0.8339 - val_loss: 0.5265 - val_acc: 0.8103\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4599 - acc: 0.8355 - val_loss: 0.5236 - val_acc: 0.8103\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4558 - acc: 0.8370 - val_loss: 0.5241 - val_acc: 0.8103\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4522 - acc: 0.8383 - val_loss: 0.5210 - val_acc: 0.8120\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4487 - acc: 0.8403 - val_loss: 0.5223 - val_acc: 0.8143\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4453 - acc: 0.8405 - val_loss: 0.5187 - val_acc: 0.8180\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4420 - acc: 0.8427 - val_loss: 0.5206 - val_acc: 0.8153\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4388 - acc: 0.8437 - val_loss: 0.5186 - val_acc: 0.8120\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4358 - acc: 0.8442 - val_loss: 0.5154 - val_acc: 0.8133\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4324 - acc: 0.8458 - val_loss: 0.5156 - val_acc: 0.8130\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4296 - acc: 0.8474 - val_loss: 0.5147 - val_acc: 0.8150\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4266 - acc: 0.8484 - val_loss: 0.5136 - val_acc: 0.8117\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4235 - acc: 0.8492 - val_loss: 0.5142 - val_acc: 0.8167\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4206 - acc: 0.8502 - val_loss: 0.5135 - val_acc: 0.8133\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4181 - acc: 0.8508 - val_loss: 0.5154 - val_acc: 0.8163\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4152 - acc: 0.8521 - val_loss: 0.5109 - val_acc: 0.8140\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4125 - acc: 0.8531 - val_loss: 0.5124 - val_acc: 0.8160\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4100 - acc: 0.8537 - val_loss: 0.5126 - val_acc: 0.8163\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4076 - acc: 0.8543 - val_loss: 0.5112 - val_acc: 0.8180\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4052 - acc: 0.8555 - val_loss: 0.5120 - val_acc: 0.8113\n",
      "Epoch 59/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4028 - acc: 0.8556 - val_loss: 0.5111 - val_acc: 0.8130\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4000 - acc: 0.8580 - val_loss: 0.5105 - val_acc: 0.8183\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3977 - acc: 0.8590 - val_loss: 0.5116 - val_acc: 0.8163\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3953 - acc: 0.8602 - val_loss: 0.5123 - val_acc: 0.8180\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3931 - acc: 0.8606 - val_loss: 0.5089 - val_acc: 0.8157\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3908 - acc: 0.8617 - val_loss: 0.5120 - val_acc: 0.8113\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3888 - acc: 0.8624 - val_loss: 0.5128 - val_acc: 0.8157\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3863 - acc: 0.8633 - val_loss: 0.5122 - val_acc: 0.8167\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3844 - acc: 0.8638 - val_loss: 0.5100 - val_acc: 0.8157\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3821 - acc: 0.8646 - val_loss: 0.5113 - val_acc: 0.8160\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3801 - acc: 0.8665 - val_loss: 0.5136 - val_acc: 0.8120\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3779 - acc: 0.8661 - val_loss: 0.5121 - val_acc: 0.8163\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3760 - acc: 0.8688 - val_loss: 0.5113 - val_acc: 0.8117\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3741 - acc: 0.8685 - val_loss: 0.5115 - val_acc: 0.8163\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3720 - acc: 0.8689 - val_loss: 0.5121 - val_acc: 0.8167\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3702 - acc: 0.8699 - val_loss: 0.5157 - val_acc: 0.8160\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3681 - acc: 0.8702 - val_loss: 0.5137 - val_acc: 0.8160\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3661 - acc: 0.8725 - val_loss: 0.5126 - val_acc: 0.8143\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3641 - acc: 0.8718 - val_loss: 0.5138 - val_acc: 0.8147\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3627 - acc: 0.8725 - val_loss: 0.5194 - val_acc: 0.8160\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3610 - acc: 0.8739 - val_loss: 0.5152 - val_acc: 0.8117\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3589 - acc: 0.8742 - val_loss: 0.5166 - val_acc: 0.8170\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3571 - acc: 0.8754 - val_loss: 0.5157 - val_acc: 0.8147\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3557 - acc: 0.8765 - val_loss: 0.5159 - val_acc: 0.8150\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3536 - acc: 0.8771 - val_loss: 0.5180 - val_acc: 0.8157\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3520 - acc: 0.8768 - val_loss: 0.5189 - val_acc: 0.8140\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3501 - acc: 0.8779 - val_loss: 0.5177 - val_acc: 0.8160\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3487 - acc: 0.8785 - val_loss: 0.5218 - val_acc: 0.8167\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3468 - acc: 0.8794 - val_loss: 0.5212 - val_acc: 0.8137\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3456 - acc: 0.8793 - val_loss: 0.5198 - val_acc: 0.8153\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3438 - acc: 0.8801 - val_loss: 0.5210 - val_acc: 0.8143\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3421 - acc: 0.8802 - val_loss: 0.5235 - val_acc: 0.8127\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3406 - acc: 0.8813 - val_loss: 0.5213 - val_acc: 0.8143\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3386 - acc: 0.8818 - val_loss: 0.5223 - val_acc: 0.8153\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3374 - acc: 0.8827 - val_loss: 0.5232 - val_acc: 0.8137\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3358 - acc: 0.8840 - val_loss: 0.5240 - val_acc: 0.8150\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3342 - acc: 0.8832 - val_loss: 0.5284 - val_acc: 0.8160\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3328 - acc: 0.8852 - val_loss: 0.5263 - val_acc: 0.8160\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3312 - acc: 0.8856 - val_loss: 0.5260 - val_acc: 0.8137\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3297 - acc: 0.8869 - val_loss: 0.5322 - val_acc: 0.8117\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3280 - acc: 0.8863 - val_loss: 0.5297 - val_acc: 0.8140\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3267 - acc: 0.8873 - val_loss: 0.5302 - val_acc: 0.8127\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3257 - acc: 0.8878 - val_loss: 0.5295 - val_acc: 0.8133\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3239 - acc: 0.8889 - val_loss: 0.5335 - val_acc: 0.8143\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3223 - acc: 0.8888 - val_loss: 0.5320 - val_acc: 0.8153\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3212 - acc: 0.8890 - val_loss: 0.5335 - val_acc: 0.8130\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3196 - acc: 0.8891 - val_loss: 0.5339 - val_acc: 0.8150\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3184 - acc: 0.8903 - val_loss: 0.5370 - val_acc: 0.8143\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3171 - acc: 0.8912 - val_loss: 0.5352 - val_acc: 0.8147\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3152 - acc: 0.8909 - val_loss: 0.5379 - val_acc: 0.8127\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3144 - acc: 0.8923 - val_loss: 0.5363 - val_acc: 0.8137\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3130 - acc: 0.8931 - val_loss: 0.5379 - val_acc: 0.8133\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3114 - acc: 0.8927 - val_loss: 0.5388 - val_acc: 0.8153\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3100 - acc: 0.8924 - val_loss: 0.5392 - val_acc: 0.8147\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3091 - acc: 0.8947 - val_loss: 0.5406 - val_acc: 0.8137\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3078 - acc: 0.8943 - val_loss: 0.5422 - val_acc: 0.8157\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3061 - acc: 0.8951 - val_loss: 0.5433 - val_acc: 0.8123\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3052 - acc: 0.8956 - val_loss: 0.5432 - val_acc: 0.8130\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3035 - acc: 0.8950 - val_loss: 0.5483 - val_acc: 0.8090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3024 - acc: 0.8965 - val_loss: 0.5461 - val_acc: 0.8117\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3013 - acc: 0.8966 - val_loss: 0.5459 - val_acc: 0.8127\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.2998 - acc: 0.8972 - val_loss: 0.5460 - val_acc: 0.8150\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 21us/step\n",
      "4000/4000 [==============================] - 0s 22us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29492314792401864, 0.8997272727272727]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5750258494615554, 0.805]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
